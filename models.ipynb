{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "dim = int(sqrt(len(df.columns)-1))\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_sample = 8\n",
    "sample = df.drop(columns='label').iloc[[ix_sample]].values.reshape(dim,dim)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2312b6cada0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIdrBaQOHqBwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2312a56e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "im = Image.fromarray(np.uint8(cm.gist_earth(sample)*255))\n",
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data, dtype=np.int16).reshape(-1)\n",
    "    return np.eye(nb_classes,dtype=np.int8)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, x_test):\n",
    "    import csv\n",
    "    csv_file = open(path+'/submission.csv', 'w', newline='', encoding='utf-8')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['ImageId','Label'])\n",
    "    pred = model.predict(x_test)\n",
    "    #ix = 155\n",
    "    #im = Image.fromarray(np.uint8(cm.gist_earth(df.values[ix].reshape(dim,dim))*255))\n",
    "    #imshow(im)\n",
    "    #print(levels[np.argmax(pred[ix])])\n",
    "    for ix, p in enumerate(pred):\n",
    "        writer.writerow([ix+1, levels[np.argmax(p)]])\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = df['label'].values\n",
    "labels, levels = pd.factorize(labels)\n",
    "classes = len(np.unique(labels))\n",
    "y = indices_to_one_hot(labels, classes)\n",
    "data = df.drop(columns=['label']).values/255\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500\n",
      "10500\n",
      "31500\n",
      "10500\n",
      "classes: 10\n",
      "[1 0 4 7 3 5 8 9 2 6]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "print(\"classes:\", classes)\n",
    "print(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple NN\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(input_shape,))\n",
    "dense1 = Dense(units=1000, activation='relu')(inputs)\n",
    "dense2 = Dense(units=1000, activation='sigmoid')(dense1)\n",
    "dense3 = Dense(units=1000, activation='sigmoid')(dense2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'NN'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some visual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv') ### 0.96785\n",
    "generate_submission(model, test_df.values/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "filters = 10\n",
    "kernel_size = 4\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_size)(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "flat1=Flatten()(max1)\n",
    "dense1 = Dense(units=1000, activation='relu')(flat1)\n",
    "dense2 = Dense(units=1000, activation='sigmoid')(dense1)\n",
    "dense3 = Dense(units=1000, activation='sigmoid')(dense2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'CNN'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values/255 ### 0.97671\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN + dropout\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "filters = 10\n",
    "kernel_size = 4\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_size)(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "flat1=Flatten()(max1)\n",
    "dense1 = Dense(units=1000, activation='relu')(flat1)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units=1000, activation='sigmoid')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(units=1000, activation='sigmoid')(drop2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'CNN+d'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values/255 ### 0.98085\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN bigger + dropout\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "filters = 100\n",
    "kernel_size = 4\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_size)(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "conv2 = Conv2D(filters = filters, kernel_size = kernel_size)(max1)\n",
    "max2 = MaxPooling2D()(conv2)\n",
    "flat1=Flatten()(max2)\n",
    "dense1 = Dense(units=200, activation='relu')(flat1)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units=400, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(units=200, activation='sigmoid')(drop2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'bCNN+d'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values ### 0.98757\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN bigger + dropout\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout, SpatialDropout2D\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "filters = 100\n",
    "kernel_sizes = [6,4]\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_sizes[0])(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "spatial = SpatialDropout2D(0.2)(max1)\n",
    "conv2 = Conv2D(filters = filters, kernel_size = kernel_sizes[1])(spatial)\n",
    "max2 = MaxPooling2D()(conv2)\n",
    "flat1=Flatten()(max2)\n",
    "dense1 = Dense(units=800, activation='relu')(flat1)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units=400, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(units=200, activation='sigmoid')(drop2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'CNN+d+Sd'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values/255 ### 0.99014\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 400)       6800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 400)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_1 (Spatial (None, 12, 12, 400)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 400)       640400    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 400)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_2 (Spatial (None, 5, 5, 400)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 400)         640400    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 400)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 800)               1280800   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 2,990,110\n",
      "Trainable params: 2,990,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(31500, 784)\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 6.6688 - acc: 0.1303Epoch 00001: val_acc improved from -inf to 0.40629, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 25s 788us/step - loss: 6.6675 - acc: 0.1302 - val_loss: 2.1965 - val_acc: 0.4063\n",
      "Epoch 2/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 2.2624 - acc: 0.1810Epoch 00002: val_acc improved from 0.40629 to 0.42800, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 659us/step - loss: 2.2624 - acc: 0.1810 - val_loss: 2.0300 - val_acc: 0.4280\n",
      "Epoch 3/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 2.0104 - acc: 0.2685Epoch 00003: val_acc improved from 0.42800 to 0.52610, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 658us/step - loss: 2.0103 - acc: 0.2686 - val_loss: 1.6787 - val_acc: 0.5261\n",
      "Epoch 4/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 1.7204 - acc: 0.3936Epoch 00004: val_acc improved from 0.52610 to 0.67114, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 659us/step - loss: 1.7202 - acc: 0.3936 - val_loss: 1.1940 - val_acc: 0.6711\n",
      "Epoch 5/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 1.4131 - acc: 0.5115Epoch 00005: val_acc improved from 0.67114 to 0.78667, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 1.4129 - acc: 0.5116 - val_loss: 0.8804 - val_acc: 0.7867\n",
      "Epoch 6/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 1.1784 - acc: 0.6120Epoch 00006: val_acc improved from 0.78667 to 0.87276, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 662us/step - loss: 1.1784 - acc: 0.6121 - val_loss: 0.6528 - val_acc: 0.8728\n",
      "Epoch 7/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.9946 - acc: 0.6925Epoch 00007: val_acc improved from 0.87276 to 0.90819, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 661us/step - loss: 0.9945 - acc: 0.6926 - val_loss: 0.4781 - val_acc: 0.9082\n",
      "Epoch 8/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.8336 - acc: 0.7503Epoch 00008: val_acc improved from 0.90819 to 0.93200, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 660us/step - loss: 0.8335 - acc: 0.7504 - val_loss: 0.3476 - val_acc: 0.9320\n",
      "Epoch 9/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.6912 - acc: 0.7973Epoch 00009: val_acc improved from 0.93200 to 0.95190, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 656us/step - loss: 0.6912 - acc: 0.7973 - val_loss: 0.2241 - val_acc: 0.9519\n",
      "Epoch 10/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.6008 - acc: 0.8318Epoch 00010: val_acc improved from 0.95190 to 0.95524, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 659us/step - loss: 0.6008 - acc: 0.8317 - val_loss: 0.1848 - val_acc: 0.9552\n",
      "Epoch 11/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.5237 - acc: 0.8560Epoch 00011: val_acc improved from 0.95524 to 0.96371, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 661us/step - loss: 0.5235 - acc: 0.8560 - val_loss: 0.1569 - val_acc: 0.9637\n",
      "Epoch 12/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.4711 - acc: 0.8745Epoch 00012: val_acc improved from 0.96371 to 0.96695, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 661us/step - loss: 0.4715 - acc: 0.8744 - val_loss: 0.1358 - val_acc: 0.9670\n",
      "Epoch 13/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.4342 - acc: 0.8833Epoch 00013: val_acc improved from 0.96695 to 0.96752, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 656us/step - loss: 0.4341 - acc: 0.8833 - val_loss: 0.1341 - val_acc: 0.9675\n",
      "Epoch 14/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.3851 - acc: 0.8968Epoch 00014: val_acc improved from 0.96752 to 0.97076, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 658us/step - loss: 0.3849 - acc: 0.8969 - val_loss: 0.1216 - val_acc: 0.9708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.3521 - acc: 0.9059Epoch 00015: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 656us/step - loss: 0.3520 - acc: 0.9060 - val_loss: 0.1265 - val_acc: 0.9691\n",
      "Epoch 16/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.9109Epoch 00016: val_acc improved from 0.97076 to 0.97371, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 661us/step - loss: 0.3353 - acc: 0.9109 - val_loss: 0.1111 - val_acc: 0.9737\n",
      "Epoch 17/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.3069 - acc: 0.9201Epoch 00017: val_acc improved from 0.97371 to 0.97581, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 660us/step - loss: 0.3070 - acc: 0.9200 - val_loss: 0.1036 - val_acc: 0.9758\n",
      "Epoch 18/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.9224Epoch 00018: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 657us/step - loss: 0.2958 - acc: 0.9224 - val_loss: 0.1094 - val_acc: 0.9743\n",
      "Epoch 19/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2685 - acc: 0.9314Epoch 00019: val_acc improved from 0.97581 to 0.97752, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 660us/step - loss: 0.2684 - acc: 0.9314 - val_loss: 0.0991 - val_acc: 0.9775\n",
      "Epoch 20/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9350Epoch 00020: val_acc improved from 0.97752 to 0.97781, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 658us/step - loss: 0.2558 - acc: 0.9350 - val_loss: 0.0977 - val_acc: 0.9778\n",
      "Epoch 21/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9365Epoch 00021: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 657us/step - loss: 0.2474 - acc: 0.9364 - val_loss: 0.0982 - val_acc: 0.9777\n",
      "Epoch 22/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2550 - acc: 0.9353Epoch 00022: val_acc improved from 0.97781 to 0.97819, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 659us/step - loss: 0.2549 - acc: 0.9353 - val_loss: 0.0929 - val_acc: 0.9782\n",
      "Epoch 23/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9406Epoch 00023: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.2287 - acc: 0.9406 - val_loss: 0.0972 - val_acc: 0.9774\n",
      "Epoch 24/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9426Epoch 00024: val_acc improved from 0.97819 to 0.97971, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 656us/step - loss: 0.2207 - acc: 0.9426 - val_loss: 0.0949 - val_acc: 0.9797\n",
      "Epoch 25/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9434Epoch 00025: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.2242 - acc: 0.9434 - val_loss: 0.0959 - val_acc: 0.9790\n",
      "Epoch 26/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9485Epoch 00026: val_acc improved from 0.97971 to 0.98048, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.2056 - acc: 0.9484 - val_loss: 0.0861 - val_acc: 0.9805\n",
      "Epoch 27/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9506Epoch 00027: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.1973 - acc: 0.9506 - val_loss: 0.0928 - val_acc: 0.9795\n",
      "Epoch 28/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9530Epoch 00028: val_acc improved from 0.98048 to 0.98105, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.1865 - acc: 0.9530 - val_loss: 0.0824 - val_acc: 0.9810\n",
      "Epoch 29/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9568Epoch 00029: val_acc improved from 0.98105 to 0.98200, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 656us/step - loss: 0.1776 - acc: 0.9568 - val_loss: 0.0787 - val_acc: 0.9820\n",
      "Epoch 30/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9575Epoch 00030: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.1707 - acc: 0.9575 - val_loss: 0.0828 - val_acc: 0.9810\n",
      "Epoch 31/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9563Epoch 00031: val_acc improved from 0.98200 to 0.98295, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.1683 - acc: 0.9563 - val_loss: 0.0767 - val_acc: 0.9830\n",
      "Epoch 32/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1591 - acc: 0.9607Epoch 00032: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.1592 - acc: 0.9607 - val_loss: 0.0783 - val_acc: 0.9824\n",
      "Epoch 33/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9628Epoch 00033: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.1516 - acc: 0.9629 - val_loss: 0.0794 - val_acc: 0.9823\n",
      "Epoch 34/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1418 - acc: 0.9649Epoch 00034: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.1418 - acc: 0.9649 - val_loss: 0.0811 - val_acc: 0.9827\n",
      "Epoch 35/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9630Epoch 00035: val_acc did not improve\n",
      "\n",
      "Epoch 00035: reducing learning rate to 3.2999999166349884e-05.\n",
      "31500/31500 [==============================] - 20s 650us/step - loss: 0.1500 - acc: 0.9630 - val_loss: 0.0768 - val_acc: 0.9827\n",
      "Epoch 36/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1313 - acc: 0.9675Epoch 00036: val_acc improved from 0.98295 to 0.98362, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.1313 - acc: 0.9675 - val_loss: 0.0795 - val_acc: 0.9836\n",
      "Epoch 37/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1179 - acc: 0.9707Epoch 00037: val_acc improved from 0.98362 to 0.98429, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.1179 - acc: 0.9707 - val_loss: 0.0724 - val_acc: 0.9843\n",
      "Epoch 38/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1139 - acc: 0.9714Epoch 00038: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.1138 - acc: 0.9715 - val_loss: 0.0737 - val_acc: 0.9829\n",
      "Epoch 39/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9708Epoch 00039: val_acc improved from 0.98429 to 0.98438, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 656us/step - loss: 0.1192 - acc: 0.9708 - val_loss: 0.0733 - val_acc: 0.9844\n",
      "Epoch 40/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9722Epoch 00040: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.1128 - acc: 0.9723 - val_loss: 0.0725 - val_acc: 0.9842\n",
      "Epoch 41/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1105 - acc: 0.9722Epoch 00041: val_acc improved from 0.98438 to 0.98476, saving model to CNN+overkill/weights.hdf5\n",
      "\n",
      "Epoch 00041: reducing learning rate to 1.0890000085055363e-05.\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.1104 - acc: 0.9722 - val_loss: 0.0726 - val_acc: 0.9848\n",
      "Epoch 42/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1057 - acc: 0.9731Epoch 00042: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.1056 - acc: 0.9731 - val_loss: 0.0713 - val_acc: 0.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9755Epoch 00043: val_acc improved from 0.98476 to 0.98495, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.0968 - acc: 0.9755 - val_loss: 0.0723 - val_acc: 0.9850\n",
      "Epoch 44/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1059 - acc: 0.9732Epoch 00044: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 651us/step - loss: 0.1059 - acc: 0.9732 - val_loss: 0.0715 - val_acc: 0.9845\n",
      "Epoch 45/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9742Epoch 00045: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.1031 - acc: 0.9742 - val_loss: 0.0701 - val_acc: 0.9850\n",
      "Epoch 46/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0992 - acc: 0.9750Epoch 00046: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.0992 - acc: 0.9750 - val_loss: 0.0700 - val_acc: 0.9850\n",
      "Epoch 47/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9736Epoch 00047: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.1047 - acc: 0.9735 - val_loss: 0.0730 - val_acc: 0.9850\n",
      "Epoch 48/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0945 - acc: 0.9770Epoch 00048: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.0944 - acc: 0.9770 - val_loss: 0.0703 - val_acc: 0.9849\n",
      "Epoch 49/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0978 - acc: 0.9756Epoch 00049: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.0978 - acc: 0.9756 - val_loss: 0.0690 - val_acc: 0.9847\n",
      "Epoch 50/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1005 - acc: 0.9749Epoch 00050: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 655us/step - loss: 0.1006 - acc: 0.9749 - val_loss: 0.0689 - val_acc: 0.9850\n",
      "Epoch 51/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9765Epoch 00051: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.0964 - acc: 0.9765 - val_loss: 0.0711 - val_acc: 0.9845\n",
      "Epoch 52/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9762Epoch 00052: val_acc improved from 0.98495 to 0.98505, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.0946 - acc: 0.9762 - val_loss: 0.0711 - val_acc: 0.9850\n",
      "Epoch 53/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9759Epoch 00053: val_acc improved from 0.98505 to 0.98533, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 656us/step - loss: 0.0957 - acc: 0.9759 - val_loss: 0.0702 - val_acc: 0.9853\n",
      "Epoch 54/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0947 - acc: 0.9756Epoch 00054: val_acc improved from 0.98533 to 0.98590, saving model to CNN+overkill/weights.hdf5\n",
      "31500/31500 [==============================] - 21s 653us/step - loss: 0.0947 - acc: 0.9756 - val_loss: 0.0671 - val_acc: 0.9859\n",
      "Epoch 55/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9755Epoch 00055: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.0950 - acc: 0.9754 - val_loss: 0.0709 - val_acc: 0.9847\n",
      "Epoch 56/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9779Epoch 00056: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 652us/step - loss: 0.0899 - acc: 0.9779 - val_loss: 0.0698 - val_acc: 0.9855\n",
      "Epoch 57/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9768Epoch 00057: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.0934 - acc: 0.9768 - val_loss: 0.0688 - val_acc: 0.9851\n",
      "Epoch 58/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9773Epoch 00058: val_acc did not improve\n",
      "\n",
      "Epoch 00058: reducing learning rate to 3.59370011210558e-06.\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.0932 - acc: 0.9773 - val_loss: 0.0693 - val_acc: 0.9857\n",
      "Epoch 59/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9792Epoch 00059: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 659us/step - loss: 0.0873 - acc: 0.9792 - val_loss: 0.0691 - val_acc: 0.9849\n",
      "Epoch 60/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9777Epoch 00060: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 657us/step - loss: 0.0874 - acc: 0.9777 - val_loss: 0.0686 - val_acc: 0.9853\n",
      "Epoch 61/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9775Epoch 00061: val_acc did not improve\n",
      "\n",
      "Epoch 00061: reducing learning rate to 1.1859210189868463e-06.\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.0907 - acc: 0.9775 - val_loss: 0.0689 - val_acc: 0.9856\n",
      "Epoch 62/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9763Epoch 00062: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.0919 - acc: 0.9763 - val_loss: 0.0685 - val_acc: 0.9859\n",
      "Epoch 63/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0871 - acc: 0.9782Epoch 00063: val_acc did not improve\n",
      "31500/31500 [==============================] - 21s 654us/step - loss: 0.0871 - acc: 0.9782 - val_loss: 0.0686 - val_acc: 0.9855\n",
      "Epoch 00063: early stopping\n"
     ]
    }
   ],
   "source": [
    "### CNN OVERKILL\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout, SpatialDropout2D\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "filters = 400\n",
    "kernel_sizes = [4,2,2]\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_sizes[0])(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "spatial = SpatialDropout2D(0.4)(max1)\n",
    "conv2 = Conv2D(filters = filters, kernel_size = kernel_sizes[1])(spatial)\n",
    "max2 = MaxPooling2D()(conv2)\n",
    "spatial2 = SpatialDropout2D(0.2)(max2)\n",
    "conv3 = Conv2D(filters = filters, kernel_size = kernel_sizes[1])(spatial2)\n",
    "max3 = MaxPooling2D()(conv3)\n",
    "flat1=Flatten()(max3)\n",
    "dense1 = Dense(units=800, activation='relu')(flat1)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units=400, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(units=200, activation='relu')(drop2)\n",
    "drop3 = Dropout(0.5)(dense3)\n",
    "dense4 = Dense(units=100, activation='relu')(drop3)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense4)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'CNN+overkill'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAFACAYAAAC/abrtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xl8XFX9//HXmTV7uiSlOwVaoKVA\nWdoCFWQTWtayXRZBQKTykyKL6Bf9CvJFRbFfFv2ySFlEEMQrIIsUBASEKlZAttJi6UIW2rRN2+yZ\nJDNzf3/cSZqGLJN0JrPk/Xw88kjvzJl7P7P05jPnfs45xnEcRERERESGOk+qAxARERERSQdKjEVE\nREREUGIsIiIiIgIoMRYRERERAZQYi4iIiIgASoxFRERERAAlxklnjNnbGOMYYw7u5+OqjDHXJiuu\nwTIYz8MYkxN7jc/sz3GNMY8bY/6cgOPPjR2/ZGf3JSLZQ+d/nf8TKVExS+98qQ4g1YwxfU3kXOY4\nzqSdOMSnwBigup+P2xdo3InjDnUJf/2MMT6gDTjXcZzHO931Ku57vCWRxxOR5NL5P2vp/C8DNuQT\nY9wPdLtZwDOx3xWx2yLdPcgYE3Acp7WvnTuOEwGq+huU4zib+/sY2W4wX7/Y56Df73E2iff/g0ia\n0fk/C+n8LztjyJdSOI5T1f4DbI3dvLnT7Zuh49LMj4wxi40xW4G/xm6/1hjzoTGm0Riz3hjzO2PM\nqPb9d72U1mn7dGPMC8aYJmPMamPM2Z3j6nopKLb938aYu4wxNbHtnxljPJ3a5BtjHjTG1Bljthpj\nfmWMudUYs7y31yCO59B+qegoY8zfjTHNxpiPjDFHddnPQcaYZcaYFmPMJ8aY+X0cd2Ss7eldbp9k\njIkaY46MbV9ojHk79rw2G2OeNcbs0ce+u75+pcaYJ2Ovd5Ux5oZuHnOCMeaN2GtXY4x51RhzYKcm\nlbHfv4+9HqEur09Jp319yRiz1BgTiu3vYWPMyE73/9wYs9wYc5YxZpUxpsEY84oxZtc+nldfMWKM\nKTLG3GmM+Tz2+q7t8lqMicWzKRbfJ8aY83t5Lr7YbefEtts/w2cbY14yxjQBNxhj/MaYB2LHazbG\nrDHG/I8xxt8lvrmxz1FT7Dm8ZoyZaIyZZ4xpNcbs0qX9N40x24wxub29NiL9pfO/zv+d2qT9+b+b\nmI0x5vvGmM9i587VxpjLu7Q50xjzQey5bzPGvGWMmR67Lxj7nLT/rVhvjPltf2LIRkM+Me6n7wBl\nwGxgQey2KHAVMB04C9gTeCSOfd0C3AfsBzwHPBzHf4rvAGuBmcB3ge8BnU+otwPHA+cAh+Fe9vlG\nHLHE+xz+F7gR2B/4GPijMaYAwBhTCLwAbIjF9w3gemBYTwd1HGcLsAS4sMtd5wPlwN9i2wHgR8AB\nwFzADzxr3Etb8XoY2AeYBxwbe64ndGmTD9yB+/5+CfdE+KIxpjh2/wGx35fh9jR1+34ZYyYAfwFW\nAwcDp+G+Jo93aborcBHue3gEMBpY3Mfz6DXG2B/KF4HjgG8CU4FLiP3Rj71fbwJ7435OpgFXAy19\nHLc7vwAexH1d7we8sXjOjh33WuBbsd/Ejn8C8DzwD+AQ3M/p73Hf078An+O+Jp19A/id4zjNA4hR\nJFF0/tf5H1J7/u/qGuCHwP/Ent8dwO3GmK/GYpkYO277eXoOcDfbr4R8BzgZOBeYAswH3ulnDNnH\ncRz9xH5w/zM4wKRu7qsCno9jH4fG9jEytr13bPvgLtvf6vSYAG5icmGX413bZdvucqzXgd/E/j0c\n90T41S5t3gOW9/N16Poc5sa2T+jUZlLsti/HthcCNUBhpzYHx9pc28ux5gOtQEmn2/4D/LiXx4yJ\n7feg2HZObPvM7l4/3JOgAxze6f5cYBPw516O48OtUzuj07YDnNOlXfvrUxLbXoT7B8zXqc3sWJtZ\nse2fx97z4Z3aXBR7D739eK+6xnhi7Dj79tD+cqAB2KWH+3d4Lt09706f4e/GEd/3gY86bb8NPNFL\n+x/g1mWa2Pb+vT0f/egnUT/o/N/Tc9D5f/t2Ss//uEnunzttbwZu6tLmHmBFp/cyCozpYX/34n6h\nMYP9/y2df9Rj3D//6nqDMeZYY8zLxpgKY0w98Ersrr6+/b/f/g/HrVGqBnbpufmOj4n5vNNj9sT9\nj/vPLm26bn9BP55D5+N/HvvdfvxpuAlQfXsDx3HeAfrq5XseqMP9xooxZnbsuTzcKb6DjDHPxC4X\n1eMmTt3F15NpuCeHjtfCcXsf/925kTFmijHmMeOWANThnuhz+3GcdvsA/3AcJ9zptn8Bodh97coc\nx9nWaftz3PdwJD2II8aDgA2O43zUwy4OAj50HGdjv55R97r7//Ct2GXPTcaYBtyenl1j9xncXpeX\netnng7H2R8a2LwWW9fJ8RAaLzv87Hht0/u9O0s7/XeIdBZQAb3S562/AFOOWsL0d2/5PrJTkCmPM\nuE5t78etqV9ljLnbGHOa6VL6NhQpMe6fHUa5GmMmA3/G/YZ7Nu435LNidwf62FfXgRsOfb8f8TzG\n6WMfO+jnc+h8/PbjtB/f9HBs09vxHcdpw72U/rXYTV8D3nIc59NYfMXAy7gnlQtxL0kd1kN8Pek1\nhk5ewD3RX4Z7mX8GUNuP43TW0/vQ+fbu3k/o/XMQT4x9fQZ6uz8a+935NevpRNn1/8MFwG24l2Hn\n4SbBt/DF16/H4ztureczwKXGrSn+Kv2/vCiSDDr/f/E4Ov93L1nn/3iO1fF8Y8n50bilde/hltl8\naoz5Suz+t3F7/6/DPfffBbxjjMnvZwxZRYnxzpmNmzRc5TjOPxzH+Q9unVAqrALCuJdOOjukj8cl\n6jl8DOzXXnMG7jd93MtcfXkYONgYsx/uyblz8f903MuE1zmO8zfHcT7B/Zbc39g8dHotjDE5wIGd\ntscBewA/cRznZcdxVuCeKDrXyEViP944jjenSw3cLNzX4uN+xt4hzhjfBcYaY/btYTfvAvubLgPc\nOtkU+z22020HdtewG0fg9u7+ynGcd2N/3HZrv9Nxr929h1sH2Zt7gdNxa6Q9wB/iPL7IYNL5fzud\n/3c8XsLP/105jrMJt5Tiy13uOgJYFfvSgeP6p+M4P3EcZw5u7/VFnfZT7zjOk47jLMT90rEf2798\nDElKjHfOKtzX8GpjzG7GmDNwayoHXeySzG+AW4w7un8vY8wi3MSkt16ERD2H3+LWRz1sjNnXGDMH\n+DVxDOqKfWtdEdtHATsmQuti+/22MWZ3Y8xxuDVccXMcZznu5ft7jTFHGGP2AR5ix5P2JtxLZ9+M\nXVKbg9vzGeq0Hwd38M3Rxp3ZoadLXr/E7Xm43xizjzHmy7jvzSux5zpQfcaIO/DuX8CTxpiTYu/p\n4caYi2P3Pxzbz3PGmKNj93/FbJ8cfyWwHrgp9hn6Mu4gu3j8BzjQGHOiMWaycUeFn9SlzU3A6caY\nRbHPyd7GmEvMjqPM/4o7XdYtwGOO42g+V0lHOv9vp/P/dsk6/3fn58B3jDEXx+JeiDvY+mYAY8yR\nxpgfGGNmGXfmn+NwS0tWxO7/vjHmXGPMNGPM7sDFuK/36gTHmVGUGO+E2If8GuBK3A/aFbgj/FPl\natzLTjZuPVUQeIwdE6cdJOo5xGrLTgDG445qfQj4Ge7JJh4P4166es5xnI7HOI6zHvcS2imx+G4e\nSHzABcAnuInjq7hJ3JJOx2nDvYQ4HfgId8T4LXxx0varcAfplLG9zm4HjuNU4vaKTsHtof0T7mty\nzgDi7rzfPmN03HlTj8dNLu/Hfc4P4fa6tL9Ph+Oe+P6Imwj/CvezguM4Lbi9Nrvi1hTeAfxXnCH+\nX2yfv8N93vsBP+nyHJ7DfS+/jFv/9k/gPNyTcXsbJxZ7AJVRSJrS+X+H/ej8v31fSTn/9+B24Ke4\nYzk+jsV3teM4j8bu34bbg/wcbm32YuAB3OcG7kDs7wHLgA9wBxLOdxxnXRJizRjtI78lSxlj/gGs\ncxznq6mORSRexphfAYc6jjMz1bGIZCqd/0X6TyvfZRFjzAG4o16X4V4m+jpuzdl/pzIukXjFBtsc\ngHtJ79IUhyOSMXT+F0kMJcbZ59u4c2WCe5n8RMdxXkthPCL98RfcEozfoUF3SWVZ1l7s+BrvDtxg\n2/YdKQpJdp7O/yI7SaUUIiJDnGVZXtyaydm2bZelOh4RkVTR4DsRETkGWKOkWESGOiXGIiJyDu5C\nCyIiQ1oqa4xVwyEimSzeFbXSmmVZAdzpsL4wf61lWQuABQC2bR80yKGJiCRan+ftVNYYO+vXr++1\nQUlJCdXV1YMUTmIo5uTLtHhBMQ+GwYx37NixkD2J8anA5bZtH9dH0z7P2aDPzWBQzMmXafFC5sU8\n2PHGe95WKYWIyNB2LiqjEBEBlBiLiAxZlmXlAV8Bnkp1LCIi6aDPGmPLsh4ETgI22bY9vZv7De7a\n4CcATcBFtm3/O9GBiohIYtm23QSMTHUcIiLpIp7Bdw8Bd+KuZd6debhrgk8BZgP3xH6LiIiIZDTH\ncQiFQkSjUYxJ36EFGzdupKWlJdVhxC0Z8TqOg8fjIScnZ8DvVZ+JsW3bb1iWNamXJqcCD9u27QD/\ntCxrmGVZY2zb3jCgiERERETSRCgUwu/34/Ol92LBPp8Pr9eb6jDilqx4w+EwoVCI3NzcAT0+ETXG\n44CKTtuVsdtEREREMlo0Gk37pFi28/l8RKPRgT8+ATF011fd7RxwXebEpKSkpNcd+3y+PtukG8Wc\nfJkWLyjmwZBp8YpIZkjn8gnp3s68Z4lIjCuBCZ22xwPdTnZp2/ZiYHFs0+lr/rpMm5MPFPNgyLR4\nQTEPhhTMYywiknRbt27l7LPPBmDz5s14vV5GjBgBwPPPP08gEOhzH1dffTWXX345kydP7rHNQw89\nRFFREaeffvpOxzx//nx+8pOfMH36F+ZsSHuJSIyfBRZalvU47qC7WtUXi4iIiOy8ESNG8PLLLwNw\n6623kp+fz2WXXbZDG8dxei0fuP322/s8zkUXXbRTcWaLeKZr+z1wJFBiWVYl8CPAD2Db9q+BJbhT\nta3Gna7t4mQFK5LxHAdCIUxLC3g84PPheDzg9UIkgmlrw7S1QWsrJhLBCQRwgkGcYBACAYhGMa2t\nHfsw4XB8xzUGxxgwxj2u17v9uO2DH8Jhd39tbZho1N1uaHCPHQy6uwmFMKEQNDe7/25r2/Fxra3u\n7bH7CYdxiouJjhhBdORIIrFeDk9DA6a+Hk99PSYUwvH5IBDA8ftx/H5MJNKxD9PcDJEITkEB0YIC\nnMJCogUFbg1X++vQ0oLJzSVQXe3G0jmmSMT9HXut2o9BMIjj99O2zz5ER49O8BstAP/8Z4D1672c\nfnpzqkMRyTrr1q3jkksuYebMmbz33ns8+uijLFq0iI8++ohQKMQpp5zC1VdfDWzvwd17773Zd999\nueCCC3j11VfJzc3lN7/5DSUlJdxyyy2MGDGCSy+9lPnz5zNr1iz+/ve/U1dXx2233cbMmTNpamri\nyiuvZN26dey5556sW7eORYsW9doz/OSTT3L33XfjOA5f+cpX+P73v084HObKK69k+fLlOI7DV7/6\nVS655BIWL17MY489hs/nY+rUqfzf//3fYL2cHeKZleLcPu53gMsTFpEMbZHI9oSpthZPXR34/UQL\nC4kWFeEUF0NBAd61a/FVVuKtqMD7+ecQjeLk5Lg/ubluMteeBLb/bk862xO4SGTHY0ejmMZGPHV1\nmLo6PLW1mIYGTFMTpqkJT1MTtLTgDBtGpLSUaGkpkVGjAPBu3Ihn0yb395Yt7v68XpxY4umJRhnd\n1IQnFBrkF3TnjEl1AP00kArjbXfeSfNppyU8FoEnn8zlr3/NUWIskiSrVq3itttu45ZbbsHn8/H9\n73+f4cOHEw6HOeusszjxxBPZc889d3hMXV0dhxxyCD/4wQ+48cYbefzxx1m4cOEX9u04Ds8//zwv\nvfQSd9xxB48++igPPvggpaWl3HfffXz88cfMnTu31/jWr1/PL37xC1544QUKCws555xzePnllxk5\nciRbt27lr3/9KwC1tbUA3HPPPSxbtoxAINBx22DTMMuhyHG296SFwxAIYBoatieR0Si098K1tGDa\n2txey4IConl5bu9hSwu+sjJ8a9fiW7sWb0WF2xOXl+cmpnl5mLY2PJs24dm8Ge/mzXi2bcPJzyda\nVES0uBinsBBaW/Fu3NiRWHq2bME43Y7d3MEunZ9OLO4vJLoDeWm8XrdHsrjYTcQLCoiOGuU+r7w8\nnEAAT00Nns2b8a1cSfCNN8BxiIwaRXTUKFoPPJDoyJEdr2P7a5xTUECzMe5rk5ODEwi470M06vZk\nRiJuIu33u0m93+/25La0uEl87L1wvF63p7O9J9nvd4/V65Ny3Nc0drz2HxOJuMdtv/zm87k9tz4f\neL0UBIM0btni9lC3tGAcZ3v87T9+//bH+f1uXJ3a4PNhamvxbtmCZ+tW90uDMUQLCzt6fhuieeT5\n2/CEW90e89ZWHJ9vhy86eL00bWxky7om6iqbaNjQiC9gyB/hp7A0QNGoAGMnjaChpQn8sVhiz6Xj\nOfn97vOMHaO5to3q9REK9htP4U5/cqQ7gQC0tGjgkmSPG24oYsUKf0L3OW1aGzfdVDegx+66667M\nmDGjY/uZZ57h97//PZFIhKqqKlatWvWFxDgnJ4ejjz4agP32249ly5Z1u+958+YBsO+++1JR4U4+\n9q9//YvLL3f7QvfZZx/22muvXuN77733mDNnTkdN9Pz581m2bBnf+ta3WL16NTfccANHH300X/7y\nlwHYc889ueKKKzj++OP7TLqTRYlxJotG8VRV4Ssvx1tWhmfrVqIjRxIdPdpN1EpL8ZaX4//gAwLv\nvYf//ffxffZZt5ff+9Mz6Ph8bs9upwQ2OmyYm4A1Nbm9su235+e7PaulpUTGj8c0N+Oprsa3di2m\nthYCASK77EJk7FhaDziAaGkp0WHD3KS0qIhoYaF7Wb2uzu3Jra8n3+ejvriYyIQJRMaPJzJ6tJv4\ntLVtv4zf0uImgu3JoOO4SVKnS+h4vV9IKp2cnL4TzQHwl5RQF8fAMMeB9eu9rF7tY/NmDxP3iLDH\nHmFGjIh2G1ZrKzQ0eKirMzQ0GOrrPRQWRhk1KsrIkVG8Xnefn33m5Z13Arz9doCPPvITDDqMGhVl\nl10ilJa6bfPzHfLyouTlOQSDDm1tw1hV18jGrV6qqrw0Nhp8Pof2PHP7v51Y3rl9282XnVh+73RU\nbHgLHTZu9PLp+z7WrPGxerWPxkYPxjgUFzsMGxZl+HA3UQ+FDC0thlAI6us91NfHN7ukMU4st3co\nKtq+z2HD3P1+/rmXykofNTXu/u66axvzd1OPZjIEAg6tramOQiR75eXldfx77dq13H///Tz//PMU\nFxdzxRVXdLuARufBel6vl0gPnUrt7Tq3ceLouOqsp/YjRozg9ddf56WXXuKBBx5gyZIl/OIXv+Cx\nxx7jrbfe4qWXXuKXv/wlr7766qDPzazEOB21f5C6ZkLRKP733yfnr38l+Oqr+Fetcus44xApLaVt\nxgxCxx/vJobtvWleL/kFBTQ2NLiJbTTq1qPm5HTUtjp+PyYUwtPYiGls7Kg7jey2G+Hddye82244\nRUXbD9bWhmlqcvefn5+gF2W73JISmrtLMmO1o05h8vr/IhFobDQdP01NHqJRYkmlm1gGg7Bli4eN\nGz1s2uRl0yYPHo+Hpqbtr4XjuAltfb3p+F1R4WXNGh9NTV9MAIcNi7LbbmG8XqirM9TVeaipMYRC\nPSeLXq9DaWmUtjbYssU9sRQWRtl//zYiEfjkEx9vvhmkrq63hHMYXq+bRBcURAmHDe0XGsJhQ1sb\nRCLub3c7vi8VY8ZEmDKljbPPbmLs2AgNDR5qatzntG2bG09pqUNODgSDDgUFUcaPjzBhgvszblyE\nUAg2bvSyaZOXjRs9tLUVUFfXRCRicMuLDfX1hpoaD9u2eSgv9+E4MHZshAMPbGbcuAjjx0eYOTNz\nVorKNG5irB5jyR4D7dkdDPX19RQUFFBYWMjGjRt5/fXXOfLIIxN6jFmzZvHcc88xe/ZsVq5cyapV\nq3ptf+CBB/KTn/yErVu3UlRUxDPPPMNll13Gli1byM/P5+STT2bixIlcd911RCIRNmzYwJe+9CVm\nzZrFU089RXNzMwUFBQl9Dn1RYpxKsR5Oz7Zt+D/6CP/77xN4/338H36IaWtze1JHjyY6ejSO10vw\nzTfxVlfjeDy0zpxJ40UXEd51VyKTJhGeOJHoyJF4qqvxbtqEZ+NGvJs3Exk9mtYZM4iOHdtjT2hu\nSQmNiZzmyu93a4EzXDjsJo8ffBDggw/8vP9+gP/8x0c4PNA/9Du+JsY4FBS4P4WFUcaMiTBrVhNT\npoSZPDlMaWmE8nK3Z3XNGh9r1/owBkaNilBcHKWoyH1ccbGbOBYVOeTnR2locJNyN2n0EI0aDjyw\nlZkzW9lzzzCeLnlwc7Nh2zZDU5Ob6Dc1GZqbDbvvXkROzhZKSqLE+4XdcdwvD9sTZYhGtyfT0ahh\n+PAoBQX963XoybhxUcC9QlFSkkd1dUNC9iuJEQw6hMOGaJQvfO5EJLH2228/pkyZwtFHH83EiROZ\nOXNmwo/x9a9/nSuvvJJjjz2W6dOns9dee1HUuWOsi7Fjx3Lttddy1llndQy+O/bYY/noo4+49tpr\nO5bZ/u///m/C4TCXX345jY2NRKNRLr/88kFPigFMf7vFE8hZv77b6Y47ZNo8qtBNzC0t+Fes2F7O\n8MEHeDdscC/5dylpaB8h37b//jh5eXiqqvBWVbntm5tpOewwWo45htCRR+IMH568mNPczsQbiUBV\nlZfyci9NTYaxY91ew8JC9//Bli0eXnstyF//msPf/hakttb9az5sWJT99mtlv/3aOsoO2ksPwE0u\nm5o8NDYaWlsNI0ZEGDUqyqhR7u+JE0ewZcuWHS4G5OY6aZ0sDKXPRX/F5jEeal2hfZ6zYcf34c47\nC/jZz4pYvXo9A1ydNeky7XMOinkwdI63qalph5KFdOXz+QjHO1PRAIXDYcLhMDk5Oaxdu5bzzjuP\npUuXDmhlwGTG2917Fu95Wz3GSWLq6ii46y7y77+/YyaCSGkpbfvvT8sRR7gDudpnUCgocBPiqVM7\npsWSgYtGYe1aL+vW+fjss/YfL2VlPiorvd1e7i8uduts163z4jiG0tIIxx8f4ogjWpgxo5VJkyI7\nVXpcUAChUMq+hIqkRCDgfuZbWw25ufr8i2S6xsZGzj777I6Etn02jGySXc8mHbS2kv/AAxTcfjve\nbdtoOu00QvPm0TZjBpFeyhmke47jDjBrbDQ0N7uX+auqDNGoh+HDox29UJs2efjb34K8/nqQN94I\nsnXr9mv/hYVRJk0Ks88+bZxwQjMTJ0aYODFCXl6Uzz/3sn69Oxhr40YPp53WxDHHtLDvvm1p3Zsr\nkgk6J8agxFgk0xUXF/Piiy+mOoykUmKcCKEQ/uXLCfz73/gfeYTA2rW0fOlLbP3hD2nbd99UR5dR\nwmFYscLPW28FWLYswLJlwY7ZA3bkLsiQk+PW2G7c6CbCJSURjjqqhTlzWthjjzC77RbpcUYHgIMP\nbuv+DhHZae0XwDQAT0QyhRLjgQqFKLztNoJLl+JfsaJjirLofvux9Xe/o+XII9U73IvWVjpmYVi9\n2s/q1T4+/dTHqlU+GhrcRHjSpDBz5zYzaVIkNuODW9M7fHghFRWNbNvmzjZQU2OYNCnCUUeFmDbt\ni4PLRCQ1/H63l7ibGaNERNKSEuMBKr7hBvIffZSWQw+lYcEC2g44gNYDDmDE9Om0ZNAAg8HQ0gJv\nvumWOaxd69b8VlZ6iUS2f3EYNSrC5MlhzjyzmZkzW5k9u4UxY7pf972kpIDq6qbBCl9EBmjHUgoR\nkfSnxHgAcv/wB/IffZT6yy+n/gc/SHU4aSUadeuB6+sN//53gBdeyOGVV3JoaPCQlxdlypQwBxzQ\nymmnRZg0Kczuu7tTkxUXq/5QJNuolEJEMo0S437yLV/OsB/8gJbDDqP+e99LdTgpsWaNl3//O0B5\nuY/ycnfqs4oKH3V1hsbGHesYRoyIcMopzZxwQog5c1rotOCOiGS57T3GKQ5EJIOdeeaZLFy4cIfF\nOu677z7Wrl3Lz372sx4fN2XKFD799FOqqqq4/vrrue+++7rd9/XXX8/+++/f437uu+8+zj//fHJj\no90vuOAC7rzzTop3cr2CRYsWkZuby2WXXbZT+0k0Jcb9YGpqGLFgAdFhw9h2993uurhZprHREAoZ\nioqi+DstB19W5uXZZ3N59tncjnXijXEYM8ad4WHOnBaGDYtSWOguNlFY6LD77mFmzmzNxpdJROKg\nUgqRnXfqqafyzDPP7JAYP/PMM1x//fVxPX706NHdJsXxuv/++znjjDM6EuNHHnlkwPvKBEpZ4hWN\nMvyqq/B+/jnVTzxBtLQ01RENWFsbfPyxn7ffDvDvfwfYvNnH+vWj2LTJQ3Pz9h7fgoIow4a5CfK6\nde5H5cADW7nxxlqOOqqFCRPCmnZZRHqkxFhk55144on84he/oKWlhWAwSEVFBRs3bmTWrFk0NjZy\n8cUXU1tbSzgc5nvf+x7HH3/8Do+vqKjgwgsv5NVXX6W5uZlrrrmGTz/9lMmTJxOKrbMAcN111/HB\nBx8QCoU48cQTufbaa3nggQfYuHEjZ511FsOHD+eJJ55g9uzZvPDCC4wYMYJ7772XP/zhDwCce+65\nXHrppVRUVHD++ecza9Ys3nnnHUaPHs2DDz7YkVh3Z/ny5Vx33XWEQiF23XVXbr31VoYNG8YDDzzA\nI488gs/nY8qUKdxzzz289dZb3HDDDQAYY3jqqacSukKeEuM45d97Lzkvv0ztj39MWxKWWUw2x4Gn\nn87l0UfzeP99f0cCPG5cmClT3IS3tDRKaWmUnByH2lpDTY2HmhoPDQ2G889v5KSTQowfH0nxMxGR\nTLG9xji1cYhkshEjRjBjxgxef/11jj/+eJ555hlOOeUUjDEEg0EeeOABCgsLqa2t5YQTTuC4447D\n9DAr1sMPP0xubi6vvPIKK1aL3lAnAAAgAElEQVSsYO7cuR33/dd//RfDhw8nEolw9tlns2LFCi65\n5BIWL17MH//4R0aMGLHDvj788ENs2+bPf/4zjuNw0kknceihh1JcXMy6deu46667WLRoEd/85jdZ\nsmQJZ5xxRo/P8aqrruLHP/4xhx56KIsWLeK2227jpptu4q677uKtt94iGAxSW1sLwK9//Wtuvvlm\nZs6cSWNjI8EE99ApMY6Db/VqihYtovmEE2i8+OJUh9NvH37o5/rri3nnnQBTprRx3nlNzJzZysEH\ntzJmTDS29GVNqsMUkSzT3mPc0qIeY8kORTfcgH/FioTus23aNOpuuqnXNvPnz+eZZ57pSIxvu+02\nABzH4ec//znLli3D4/FQVVXF5s2bGTVqVLf7WbZsGV//+tcBmDZtGlOnTu2477nnnuPRRx8lEomw\nceNGPv30U6ZNm9ZjTP/617+YO3dux9LL8+bNY9myZRx33HFMmDCB6dOnA7DffvtRUVHR437q6uqo\nra3l0EMPBeCss87im9/8JgBTp05l4cKFzJ07tyOJnzlzJv/zP//Daaedxrx589qXek4Yzfjal2iU\n4muvxcnNpfbmmzNqbuLNmz1ce20xJ5xQwmefebn11m28+upmbrqpjpNPDvU4HZqISCKolEIkMebO\nncvSpUv56KOPCIVC7BtbPOypp55iy5YtvPDCC7z66quUlJTQ0sfE4d31JpeXl3eURbzyyiscc8wx\nO5RZdMdxep5NqnMvrtfrJRIZ2NXmhx9+mIsuuogPP/yQuXPnEg6HWbhwIYsWLSIUCnHyySezevXq\nAe27J+ox7kPeb39L8O232XbHHWldV7xli4c//zmH1at9HT/r1/vw+RwWLGjkqqvqKSrSlGgiMng0\nXZtkm756dpMlPz+fQw89lGuuuYb58+d33F5fX09JSQl+v5+lS5dSWVnZ635mz57Nn/70J+bMmcMn\nn3zCypUrO/aTm5tLUVERmzdv5rXXXuvowS0oKKChoeELpRSHHHIIV199NQsXLsRxHF588UV+9atf\n9fu5FRUVUVxczLJly5g9ezZPPvkkhxxyCNFolPXr1zNnzhxmzZrF008/TWNjI9u2bWPq1KlMnTqV\nd999l9WrVzN58uR+H7cnSox74a2spOjmmwkdeSTNZ56Z6nC6FQ7Dww/n87//W0htrYf8/CiTJ4c5\n5JBW9tijiZNPbmaPPVQXLCKDb3spRYoDEckC8+fP5xvf+Ab33HNPx22nn346F154IfPmzWP69Ol9\nJohf+9rXuOaaazj22GOZNm0aM2bMAGCfffZh+vTpHHXUUUycOJGZncZSffWrX+X8889n1KhRPPHE\nEx2377vvvpx11lmceOKJgDv4bvr06b2WTfTkjjvu6Bh8N3HiRG677TYikQhXXHEF9fX1OI7DpZde\nSnFxMYsWLeIf//gHHo+HPffck6OOOqrfx+uN6a0rPMmc9evX99rArX1N0SpyjsOI888n8PbbbH71\nVSLjx8f1sMGM+a23Alx/fTErV/o54ogQ119fx9Sp4X5Xe6T0dR6ATIsXFPNgGMx4YzVtQ60btM9z\nNuz4PtTWGqZNG8OPflTLggWNyY5vQDLtcw6KeTB0jrepqamjjjad+Xw+wuFwqsOIWzLj7e49i/e8\nrR7jHuQ+8QQ5r79OzU9/GndSPBjq6w2vvhrk6adzeemlXMaPD3P//VuZOzeUSeXPIjIEqMZYRDKN\nEuNumLo6im+8kZaZM2n62tdSHQ7NzfD003ksWZLD0qVBWlsNpaURrrmmnm99q4HcXNUOi0j/WZY1\nDLgfmA44wNdt234rUftvX+lS07WJSKZQYtyNvMcfx1NT4xbZe1I3cYfjwJIlOdx0UxGVlT4mTgxz\n0UWNnHBCiAMPbMXrTVloIpIdfgm8aNv2mZZlBYCEXi/2esHrdTRdm4hkDCXGXUUi5D/4IC2zZ9O2\n334pC2PVKh/XX1/M0qVBpk5tw7arOeywVpVLiEhCWJZVBBwBXARg23YrkPC+3UDAUSmFZLQUjsWS\nAdqZ90yJcRc5L7+Mr6KCujjXIE+0SAQWLSrknnsKyM93+OlPazj//CZ8eqdEJLF2BzYDv7Esa3/g\nXeBK27Y7RslZlrUAWABg2zYlJSV97tTn8+3QLifH4PHkUlISSHD4idE13kygmJOvc7zGGKLRKH6/\nP8VR9c2XYclCMuJta2ujoKCAkSNHDujxmfUKDoL8++8nPH48oS5rjQ+G5mb49reHs2RJLpbVxA9/\nWMfIkVqEQ0SSwgccCFxh2/Yyy7J+CVwHdPQK2La9GFgc23TimVWg6+wDfv8u1NeHqK6uTWTsCZNp\nsyWAYh4MneN1HIdQKERTU1OPSy2ng2Aw2OfiHukkGfE6joPH4yEnJ+cLn7d4V8hTYtyJb/lygm+9\nRe311zPYXbRbtxouvngk777rT+upjUQka1QClbZtL4ttP4GbGCdUIKAaY8lsxhhyc3NTHUafMvnL\nRzrRktCdFDzwANG8PJrOPXdQj1tW5uXUU0v56CM/v/71NiXFIpJ0tm1XARWWZe0Vu+kYYEWijxMI\naLo2Eckc6jGO8WzeTO7TT9N03nk4xcWDcsz2WSd+8INiwmHDH/6whZkzNa+RiAyaK4BHYzNSrAUu\nTvQBgkFH07WJSMZQYhyT98gjmNZWGr7+9UE53vLlPm68sZi33gqy995t3HvvViZP1tLNIjJ4bNt+\nHzg4mcfQrBQikkmUGAO0tJD/298SOvpoInvskdRDbd7s4ZZbCnn88TyGD4/y85/XcO65mnVCRLKT\naoxFJJMoHQNyn3sOb3U1NZdemtTjbNni4ZRTStiwwcuCBY1ceWU9xcWaH1FEspdbY5zqKERE4qPE\nGAguXUqktJSWww9P2jFaWuCSS4azaZOXJ5+s5qCD2pJ2LBGRdBEMOjQ0aJy3iGQGJcaAb+VK2vbZ\nh2QtK+c48L3vDePtt4PcffdWJcUiMmSoxlhEMom+xofD+FetIjx1atIOcffdBTzxRB7f+U4dp54a\nStpxRETSTSCAaoxFJGMM+cTYt3YtprWVtiQlxi++mMPPflbIqac2cfXVDUk5hohIunJ7jFMdhYhI\nfJQYr1wJkJTEePVqHwsXDmPGjDZuvbUmWZUaIiJpy53HWCc/EckMQz4x9q9YgePzEZ48OaH7jUbh\nu98tJhiEBx7YSgasJikiknB+v1a+E5HMMeQH3/lXriQ8ZYpbCJdAjz2Wx7/+FeTWW7exyy7RhO5b\nRCRTuPMYpzoKEZH4DPkeY9/KlQkvo9i40cNPf1rEYYe1cPbZzQndt4hIJtGsFCKSSYZ0YmxqavCt\nX5/wGSl++MNiWloMt9yiumIRGdqCQYdIxBDRivcikgGGdGLs/+QTANr23jth+3z2WcOSJblcfXU9\nu++uvwQiMrS1V6lpZgoRyQRDOjFO9IwU9fWGq67yMXVqG5ddpqnZREQCAXfZe5VTiEgmGNKD7/wr\nVxIdNozo6NEJ2d///m8h69fDvffW4PcnZJciIhltx8TYSW0wIiJ9GNI9xv4VK9ze4gQUAm/davjd\n7/K44IIoBxygJZ9FRACCQfe3eoxFJBMM3cQ4GsX3ySe0TZuWkN098kg+oZCHq67S1GwiIu3ae4w1\nZZuIZIIhmxh7y8rwNDcnZEaKlhZ46KF8jjwyxD776FKhiEg71RiLSCYZsomxP4ED755+OpdNm7x8\n85uNO70vEZFsEgwqMRaRzDGkE2PHGMJ77bVT+3EcWLy4gKlT2zj8cF0rFBHprH26NpVSiEgmiGtW\nCsuy5gK/BLzA/bZt/7zL/ROB3wLDYm2us217SYJjTSjfJ58Q2W03nNzcndrP3/4W5JNP/Nx++zYt\n5iEi0oVKKUQkk/TZY2xZlhe4C5gHTAPOtSyr64i1HwK2bdsHAOcAdyc60ETrmJFiJ917bz677BJh\n/nwt/Swi0pUSYxHJJPGUUswCVtu2vda27VbgceDULm0coCj272JgfeJCTDzT2Ii3rGynE+OVK328\n8UYOF1/c2HG5UEREttu+8p0SYxFJf/GUUowDKjptVwKzu7S5EXjJsqwrgHzg2IRElyS+//wH4ziE\nd3KqtsWLC8jNjXL++Rp0JyLSHU3XJiKZJJ7EuLuv+V3nJDsXeMi27VstyzoUeMSyrOm2be8wqa9l\nWQuABQC2bVNSUtJ7cD5fn20GwlPh5vkFhx1GwQD3v2kT/OlPfi65JMqUKSM7bk9WzMmUaTFnWryg\nmAdDpsU7VKiUQkQySTyJcSUwodP2eL5YKnEJMBfAtu23LMvKAUqATZ0b2ba9GFgc23Sqq6t7PXBJ\nSQl9tRmIorffJi8/n+r8fBjg/n/3uzza2oZxxhlbqK4Od9yerJiTKdNizrR4QTEPhsGMd+zYsYNy\nnGzQvvJdW5sSYxFJf/Ekxm8DUyzL2g34HHdw3Xld2pQDxwAPWZY1FcgBNicy0ETyr1xJeO+9wTPw\n2eqefz6X3XcPM3VquO/GIiJDlEopRCST9JkZ2rYdBhYCfwFWujfZH1uWdZNlWafEmn0HuNSyrA+A\n3wMX2badnkvAOQ7+lSt3auDdli0e/vGPACed1Kwp2kREeqFSChHJJHHNYxybk3hJl9tu6PTvFcCc\nxIaWHJ6tW/HU1hKePHnA+3jxxRyiUcOJJ2qKNhGR3igxFpFMEldinE28ZWUAhHfddcD7eP75HCZN\nCrPPPiqjEJHMZVnWZ0A9EAHCtm0fnOhjtNcYt7Ymes8iIok35BJjX3k5AJEBJsZbtxqWLg3y//5f\ng8ooRCQbHGXbdtJGLXo84PM5tLTohCki6W/go88yVHuPcWTixAE9/qWXcohEDCeeGEpkWCIiWSsQ\ncFRKISIZYcglxr6yMiKjRuHk5g7o8c8/n8vEiWH23bctwZGJiAw6B3dxpndj88wnRSCgGmMRyQxD\nrpTCW14+4PrimhrDm28G+cY3GlVGISLZYI5t2+styxoFvGxZ1ie2bb/Rfmd/F2WC7hdayc01eDw5\nlJT4Ext9AmTiwjCKOfkyLV7IvJjTNd6hlxiXldF66KEDeuxLL+XQ1qbZKEQkO9i2vT72e5NlWX8C\nZgFvdLq/X4syQfcLrfh8o6itbaW6uiZRoSdMpi1kA4p5MGRavJB5MQ92vPEuzDS0SilaWvBu2DDg\ngXfPP5/LuHFhZsxQGYWIZDbLsvItyyps/zdwHLA8GcdSjbGIZIoh1WPsrajAOA7hAQy8q6szvPFG\nkIsuUhmFiGSFXYA/WZYF7t+Cx2zbfjEZB3JrjJOxZxGRxBpSiXHHVG2TJvX7sS+/nENrq8ooRCQ7\n2La9Fth/MI6lHmMRyRRDqpTCG0uMB9Jj/Je/5DB6dIQDD1QZhYhIfwQCmsdYRDLDkEqMfWVlRHNy\niI4a1a/HRSLw978HOeKIFjxD6hUTEdl5mq5NRDLFkErzvOXl7sC7fhYJf/yxn5oaD0cc0ZKkyERE\nslcw6NCmi20ikgGGVGLsKysb0Ip3b7wRBOBLX1JiLCLSX6oxFpFMMXQSY8fBW1Y2oPriN98MMnVq\nG6Wl0SQEJiKS3QIBVGMsIhlhyCTGni1b8DQ19XsO4+ZmePvtAIcfrt5iEZGBcHuMUx2FiEjfhkxi\n7C0rA+j3ctBvvx2kpcWovlhEZICCQZVSiEhmGDKJsS+WGPe3x/iNN4IEAg6zZ6u7Q0RkIFRjLCKZ\nYsgkxh09xuPH9+txb74Z4KCDWsnLc5IRlohI1nNrjFMdhYhI34ZMYuwrLycyejTk5sb9mK1bPSxf\nrvpiEZGdoR5jEckUQyYx9paX97u++M03AwBKjEVEdkIw6BCJGCKRVEciItK7IZMY+z77rN9zGC9d\nGqSoKMr++2tmehGRgQq4fQzqNRaRtDc0EuNQCG9VVb96jB3HHXg3Z04LXm8SYxMRyXJ+vztGQ3XG\nIpLuhkRi7KusBOhXj/Fnn3mprPSpjEJEZCcFAm5irB5jEUl3QyIxHsgcxu3LQCsxFhHZOUH3dKrE\nWETS3pBIjAcyh/HSpUHGjw+z224aLSIisjPae4xVSiEi6W5IJMbesjKiublES0riah+JwN//HuTw\nw1sw6uAQEdkp7YlxW5tOqCKS3oZGYlxe7vYWx5nlrlnjo7bWo9XuREQSIBhUjbGIZIYhkRj7+jmH\n8fLlfgD23VfTtImI7Kz26dpUSiEi6S77E2PHwVtW1q8ZKT76yE9OjsPkyeEkBiYiMjRoVgoRyRRZ\nnxh7Nm/G09zc7x7jvfduw+dLYmAiIkOEEmMRyRRZnxi3T9UWb4+x48DHH/vZZx+VUYiIJIKmaxOR\nTJH1ibGvvByIfw7jykovtbUepk9XYiwikgiark1EMkXWJ8beigoAIuPHx9VeA+9ERBJLpRQikimy\nPzHesIHIiBGQkxNX++XL/Xi9DnvvrcRYRCQRlBiLSKbI/sS4qoro6NFxt1++3M/kyWFyc5MYlIjI\nELK9xji1cYiI9CXrE2NPVRWRfibGGngnIpI422uM1WMsIukt6xNjb1UVkTFj4mpbXe2hqsqrgXci\nIgnk96uUQkQyQ3bP1NvSgre6Ou7EuH3gnRJjERkKLMvyAu8An9u2fVKyjqPp2kQkU2R1j7F30yaA\nuGuM2xNjlVKIyBBxJbAy2QfxeMDnc1RjLCJpL7sT46oqgH71GE+YEGbYMCeZYYmIpJxlWeOBE4H7\nB+N4gYCjHmMRSXtZnRh71q8HiHvw3fLlfpVRiMhQcQfwPSA6GAcLBFRKISLpL6trjDt6jONIjOvr\nDevW+TjjjKZkhyUiklKWZZ0EbLJt+13Lso7spd0CYAGAbduUlJT0uW+fz9dtu9xcg8eTQ0mJf8Bx\nJ0NP8aYzxZx8mRYvZF7M6Rpv1ifG0ZwcnOLiPtuuWKEV70RkyJgDnGJZ1glADlBkWdbvbNs+v3Mj\n27YXA4tjm051dXWfOy4pKaG7dj7fKGprW6murtnp4BOpp3jTmWJOvkyLFzIv5sGOd+zYsXG1y/7E\neMwYMH1fvtOMFCIyVNi2/X3g+wCxHuNruybFiaYaYxHJBNldY7xhQ7/qi0tKIuyyy6CU24mIDClu\njXGqoxAR6V3W9xi3zpwZV9v2gXdxdC6LiGQN27ZfB15P9nGCQfUYi0j6y94e42gU78aNcU3V1tIC\nq1b5VEYhIpIkgYCjJaFFJO1lbWLs2boV09oa1+Ieq1b5CYeNFvYQEUkSTdcmIpkgexPjfkzV9tFH\nGngnIpJM7uC7VEchItK7uGqMLcuaC/wS8AL327b9827aWMCNgAN8YNv2eQmMs9+8GzYA8SXGn3zi\nIy8vyqRJkWSHJSIyJKnGWEQyQZ89xpZleYG7gHnANOBcy7KmdWkzBXfqnzm2be8DXJWEWPulIzGO\no8Z49Wofe+wRxpO1/eciIqnl96MaYxFJe/GkgrOA1bZtr7VtuxV4HDi1S5tLgbts294GYNv2psSG\n2X/eqiocj4doaWmfbdes8TF5cngQohIRGZpUSiEimSCeUopxQEWn7Upgdpc2ewJYlvV33HKLG23b\nfjEhEQ6Qt6qK6KhR4Ov9KTY3Gz7/3Ms55ygxFhFJFpVSiEgmiCcx7u5M5nSznynAkcB44E3Lsqbb\ntr3D2p+WZS0AFgDYtt3nGtk7s462b8sWmDChz8d/+KHBcQwzZuRRUpIzoGPtcNw0Xfu7N5kWc6bF\nC4p5MGRavEONVr4TkUwQT2JcCUzotD0eWN9Nm3/att0GrLMs6z+4ifLbnRvZtr0YWBzbdPpaI3tn\n1tEuLS8nvMcebOvj8e+8kwOMYNSorVRX73yvcaatVQ6ZF3OmxQuKeTAMZrxjx44dlONkE618JyKZ\nIJ7E+G1gimVZuwGfA+cAXWeceBo4F3jIsqwS3NKKtYkMtL+8VVW0zJnTZ7s1a9yXYPfdNSOFiEiy\nqMdYRDJBn4PvbNsOAwuBvwAr3Zvsjy3LusmyrFNizf4CbLEsawXwGvBd27a3JCvovpjGRjx1dXEt\n7rFmjY9x48Lk5natDhERkUQJBh0iEUNEfRAiksbimsfYtu0lwJIut93Q6d8OcE3sJ+U8/ZiqTTNS\niIgkXyDg/m5tNeqIEJG0lZUz93rjXPXOcdzEeI89lBiLiCRTIOAmwy0tKQ5ERKQXQzoxrqry0Njo\nUWIsIpJk7Ymx6oxFJJ1lZ2IcK6WI9lFK0T7wTomxiEhyBYNKjEUk/WVnYlxVRbS4GCcvr9d2q1cr\nMRYRGQztNcYqpRCRdJaVibGnqqrPMgqAtWt95OVFGTMmOghRiYgMXSqlEJFMkJWJsTfOxLh94J3R\neVpEJKmUGItIJsjOxHjDhrimalu9WjNSiIgMhs7TtYmIpKvsS4zDYTybN/e5uEdzM3z+uVdzGIuI\nDAJN1yYimSDrEmPPpk2YaLTPUop163w4jmH33ZUYi4gkm0opRCQTZF1i7I1z1bv2GSnUYywiknzB\noPtbibGIpLPsS4zjXNyjfQ7j3XePJD0mEZGhbnuPcYoDERHpRdYmxvEs7jFuXJjcXGcwwhIRGdJU\nSiEimSDrEmNPVRVOIEB0xIhe261Z41MZhYjIIFFiLCKZIOsSY++GDW4ZRS+TEzvO9jmMRUQk+bbX\nGKc2DhGR3mRfYhzH4h5VVR4aGz1KjEVEBsn26drUYywi6Sv7EuMNG/qcw7h94J0SYxGRwaFSChHJ\nBNmVGIfDeNevJzx+fK/NlBiLiAwurXwnIpnAl+oAEslbXo5pbSU8eXKv7das8ZGXF2XMmOggRSYi\nkl4sy8oB3gCCuH8LnrBt+0fJOp7HA36/oxpjEUlrWdVj7Fu9GiCuxHiPPcK9jc8TEcl2LcDRtm3v\nD8wA5lqWdUgyDxgIOKoxFpG0llU9xr41a4D4EuODDlK3hYgMXbZtO0BDbNMf+0nqxO6BgKNSChFJ\na1mVGPs//ZTIqFE4xcU9tgmFoLLSy1lnacU7ERnaLMvyAu8Ck4G7bNtelszjBQKark1E0ltWJca+\n1asJ77FHr20qK704jmHXXTXwTkSGNtu2I8AMy7KGAX+yLGu6bdvL2++3LGsBsCDWlpKSkj736fP5\nemyXk+PBmBxKSvwJiT8Reos3XSnm5Mu0eCHzYk7XeLMnMXYcfGvW0Hzyyb02q6hwn/LEieoxFhEB\nsG27xrKs14G5wPJOty8GFsc2nerq6j73VVJSQk/tfL5S6urCVFdv2+mYE6W3eNOVYk6+TIsXMi/m\nwY537NixcbXLmsF3nupqPDU1fdYXl5V5AZg4UT3GIjJ0WZZVGuspxrKsXOBY4JNkHlOlFCKS7rKm\nx7hjRoopU3ptV1HhIyfHYdQoTdUmIkPaGOC3sTpjD2Dbtv3nZB4wGNTgOxFJb9mXGPfRY1xe7mX8\n+DCerOkrFxHpP9u2PwQOGMxjalYKEUl3WZMe+j79lGhuLpExY3ptV17uVX2xiEgKuKUUSoxFJH1l\nT2K8Zo3bW9xHV3B5uU+JsYhICrg9xqmOQkSkZ9mTGK9e3WcZRU2Noa7Oo4F3IiIpoBpjEUl3WZEY\nm+ZmfJWVfc5hXF6uqdpERFJFS0KLSLrLisTY274UdB8zUmiqNhGR1NF0bSKS7rIiMfbHOSOFFvcQ\nEUkdzUohIukuKxJj3+rVOB4P4d1267VdWZmX4cMjFBY6gxSZiIi0U42xiKS77EiMP/2UyMSJEAz2\n2q6iQlO1iYikSiAALS2pjkJEpGfZkRi3T9XWh7IyTdUmIpIqKqUQkXSX+YlxJIJv7do+E+NIBD7/\n3Muuu2rgnYhIKgQCDtGoIazTsIikqYxPjL0VFZiWFtr6mJGiqspLW5thwgT1GIuIpEIg4P5Wr7GI\npKuMT4x97TNS9DmHsaZqExFJpUDAHfisOmMRSVfZkxj3UUqxPTFWj7GISCq0J8bqMRaRdJUViXGk\npARn+PBe25WV+fB4HMaNU2IsIpIKwaCbGLe1KTEWkfSUFYlxPDNSVFR4GTs2gt8/CEGJiMgXtNcY\nq5RCRNJVdiTGfdQXg6ZqExFJNZVSiEi6y+jE2LN1K95t2wj3MSMFtC/uoYF3IiKposRYRNJdRifG\nvk8/BfoeeNfcbNi0SaveiYikUvvipEqMRSRdZXRinPv00wC07bVXr+0qKjQjhYhIqmm6NhFJdxmb\nGAdffpn8hx+mYcEComPH9tq2rExzGIuIpJpKKUQk3WVkYuzZtIlh3/kObVOnUnfddX22r6jwAbDr\nruoxFhFJlfbp2pQYi0i6yrzE2HEYds01eBob2Xb33duL1npRVuYlNzfKyJHRQQhQRES6o+naRCTd\nZVxinP/gg+S89hq1119PeM8943pMRYWXXXeNYNRJISKSMiNHRvF6Hd57L5DqUEREuuWLp5FlWXOB\nXwJe4H7btn/eQ7szgT8CM23bfidhUcb4Vq6k6Kc/JXTMMTRdeGHcjysv9zFhgsooREQGVTSKZ9Mm\noqNHA25ifOqpzTz6aB7f/nY9I0Y4KQ5QRGRHffYYW5blBe4C5gHTgHMty5rWTbtC4NvAskQHCUBr\nK8MXLiRaVETNbbcRb/ev47ilFBp4JyIyuIq/+11KzjgD09jYcdvllzfQ1OThoYfyUxiZiEj34iml\nmAWstm17rW3brcDjwKndtPsx8AsglMD4tgsEaLj8cmruuINoSUncD9u61UNTk0dTtYmIDLJmy8Jb\nXk7x9dd33Lb33mG+8pUQDzxQQGOj6ttEJL3EkxiPAyo6bVfGbutgWdYBwATbtv+cwNi+oPn002k5\n8sh+PUZTtYmIpEbr7Nk0XHEFeX/4AznPPttx+8KF9dTUeHj00bwURici8kXx1Bh395W+ozDMsiwP\ncDtwUV87sixrAbAAwLZtSvro+fX5fH226UtNjZv7779/ESUlya9nS0TMgy3TYs60eEExD4ZMi3eo\nqL/6aoJvvMGw//ovNh90EJFx4zj44DYOPbSFe+8t4MILG+OZXEhEZFDEkxhXAhM6bY8H1nfaLgSm\nA69blgUwGnjWsqxTuk2ZYkAAACAASURBVA7As217MbA4tulUV1f3euCSkhL6atOXjz8uAIooLKym\nujr5iXEiYh5smRZzpsULinkwDGa8Y/tYVCgTWJY1AXgY95wdBRbbtv3LhB/I72fbnXdSetxxDLvi\nCrb88Y/g9XLFFQ2cd95Innoqj3PPbUr4YUVEBiKeUoq3gSmWZe1mWVYAOAfouCZm23atbdsltm1P\nsm17EvBP4AtJcapUVHgZOTJCXp5GP4uIdBIGvmPb9lTgEODy7gZWJ0Jk0iRqb76Z4LJlFNx5JwBH\nHNHCvvu2ctddBUQ0BERE0kSfibFt22FgIfAXYKV7k/2xZVk3WZZ1SrID3FmVlV7Gj9dZV0SkM9u2\nN9i2/e/Yv+txz+/jen/UwDWfcQZN8+dTeOut+JYvxxhYuLCBdet8LFmSk6zDioj0S1zzGNu2vQRY\n0uW2G3poe+TOh5U4lZVe9t5bA+9ERHpiWdYk4ACSNd0mgDHU3nwzOS++SP5jj1F7883Mmxdi993D\n3H13ASedFNIiTCKScnElxpnKceDzz3185Staf1REpDuWZRUATwJX2bZd1+W+fg2Yhj4GQZaU4Jx4\nInkvvID/7rvB5+PKK+HKKwN89lkpM2cOfslbJg7aVMzJl2nxQubFnK7xZnVivHmzh1DIMH68eoxF\nRLqyLMuPmxQ/atv2U13v7++Aaeh7EGTOcccx4sknqXvuOVoPP5zjjzfk5+/Cr37Vxu231wzsieyE\nTBtkCop5MGRavJB5MQ92vPEOmo5n8F3Gqqx05zBWjbGIyI4syzLAA8BK27ZvG6zjthxzDNG8PHKf\new6AwkKH009v5tlnc9m2TbUUIpJaWZ0YV1S4ifGECUqMRUS6mANcABxtWdb7sZ8Tkn1QJzeX0HHH\nkfv889DWBsDXvtZIKGT44x+14IeIpFZWl1JUVrpPTz3GIiI7sm17Kd0v4JR0oVNOIe/ppwkuXUrL\nUUcxbVqYgw9u5eGH8/nGNxrxZHWXjYiks6w+/VRWehk2LEpBgeYwFhFJF6EjjyRaWEhup2WiL7yw\nkXXrfCxdGkhhZCIy1GV9YqyBdyIiaSYYJHT88eS8+CK0uLMGnXhiMyNGRHjkkfwUByciQ1lWJ8YV\nFV7VF4uIpKHmU07BU1dH8I03AAgG4ZxzmvjLX3LYsCGr/zSJSBrL2rOP42jVOxGRdNVy+OFEhw3b\noZzi/PObiEbh97/XIDwRSY2sTYy3bvXQ3OxRYiwiko4CAZrnzSPnpZcgFAJg110jHHVUC48+mt8+\nYYWIyKDK2sS4fQ5jlVKIiKSn0Cmn4GloIOe11zpuu+CCRqqqvLzySk4KIxORoSprE+P2OYw1+E5E\nJD21HHYYkREjdiinOProFkaPjqicQkRSImsTY616JyKS5nw+QscdR/Bvf4NIpP0mzjqriddeC2oQ\nnogMuqw961RWeikqilL8/9u78/AoqnTx49/q6iWdPZ0GgxJglEWBuLK6gcimuDAqBbjyA7e54jCg\nOMqVWXBmXK+D2zgKg4jKwJFBGRURR/E6jooDc+e6wRUMMwSBQDay91b1+6M6IYEAAZP0wvt5nno6\nXTlV/aa7cvqtU6fOyZIxjIUQIl4Fhw7FsW8fzs2bG9dNmlSLaWooJa3GQoiOlbSJcVGRU1qLhRAi\nzgWHDgXA8+mnjet69Ihw7rkBli9PxTRjFZkQ4niUtImxTO4hhBDxL3LSSYTz83E3SYwBJk+u5d//\ndvLJJzITnhCi4yRlYtwwhrGMSCGEEPEvOGSInRhb+7u+XXJJHZmZptyEJ4ToUEmZGFdUaFRXyxjG\nQgiRCAJDh6KXleH85pvGdV4v/PCHdaxe7aWiQothdEKI40lSJsY7djgBGZFCCCESQXDIEADcn3zS\nbP2119YQCGi8/ro3FmEJIY5DSZoYy+QeQgiRKCLduhHp0qXZDXgA/fuH6d8/yNKlaTGKTAhxvEnK\nxFgm9xBCiASiaQRa6GcM9k14X33l4osvXDEKTghxPEnKxHjHDp20NJPsbBnDWAghEkFwyBD0vXvR\nv/222frx4+vweCyWLpWb8IQQ7S9pE+P8/Aia3K8hhBAJIRDtZ3xgd4rsbIvx4+tQKpXi4qT8yhJC\nxJGkrGVkcg8hhEgskVNOIdKp00HjGQP8+MdVhELwzDPpMYhMCHE8ScrE2J7cQxJjIYRIGJpGcMgQ\nPJ98clA/4x49IhhGLS+/nMbOnUn5tSWEiBNJV8Ps26dRWekgP19uvBNCiEQSGDIEffdu9O3bD/rd\njBnVmCY8/XRGDCITQhwvki4xbhiqTVqMhRAisQSHDgVosTtFfn6ESZNqWbo0le++0zs6NCHEcSIJ\nE2OZ3EMIIRJRuHdvIj6f3Z2iBXfeWYWmwRNPSF9jIUT7SMLEWCb3EEKIhBTtZ9xSizHASSeZXHdd\nDcuXp7J9u7QaCyHaXtIlxkVFOl6vic9nxjoUIYQQRyk4ZAjOoiL0775r8ffTp1ej6zB/vvQ1FkK0\nvaRLjBtGpJAxjIUQIvEELrgAgJR33mnx93l5JjfcUMOKFV6++cbZkaEJIY4DSZkYSzcKIYRITOHe\nvQkWFOBdvvyQZX7842oyMizuvjubiFT3Qog2lFSJsWXB9u0yuYcQQiSy2okTcX/5Jc4vv2zx97m5\nJvPm7WPjRjcvvpjWwdEJIZJZUiXGu3Y52LfPQZ8+oViHIoQQ4hjVjR+P5XaTqtQhy1x1VR0jRtTz\n4IMZFBXJjXhCiLaRVInx5s0uAE49VSb3EEKIwzEMY5FhGHsMw2i5WTaGrJwc6seMwbtyJQQCLZbR\nNHjwwX1oGvz0p1kHTpYnhBDHJEkTY2kxFkKII1gMjI11EIdSO3Eienk5Ke++e8gyXbtGmDOnkv/+\n7xRefdXbgdEJIZJVUiXGmzY56dIlQna2NB0IIcThKKU+BMpiHcehBC68kEheHqmHuQkP4MYbaxk4\nMMAvf5nFnj1J9ZUmhIiBpKpFNm1ycdpp0loshBAJT9epnTABzwcf4Ni165DFHA547LEK6uo06VIh\nhPjekmYQyFAItm51ctFF9bEORQghkoJhGLcCtwIopfD7/Ufcxul0tqpcq9x+O9pTT+FfswZz9uxD\nFvP74YEHItxzjxelXNxxR+sneGrTeDuIxNz+Ei1eSLyY4zXepEmMCwudhEKa3HgnhBBtRCn1PPB8\n9KlVUlJyxG38fj+tKdcq2dnkDh6MvmgRJVOmcLiZm669Ftau9XHvvR769i2noKB1Vw/bNN4OIjG3\nv0SLFxIv5o6O98QTT2xVuaTpSrF5s53jy413QgiRPGonTsRZWIh7w4bDltM0+O1vy8nNNbn99hyq\nqmT6UyHE0UuaxHjTJhdOp0XPntJiLIQQR2IYxh+BT4A+hmHsMAxjWqxjakn9ZZdhpqaS+sc/HrGs\nz2fxzDPlbN+uc++90t9YCHH0kqYrxebNLk45JYzHE+tIhBAi/imlJsc6htaw0tKomzCB1FdeoWrG\nDCLdux+2/ODBQe6+u4pHHsnk/PODTJ5c20GRCiGSQRK1GDulG4UQQiShqjvvBKeTjMcfb1X56dOr\nOf/8APffn8kXX7jaOTohRDJJisS4qkpjxw6n3HgnhBBJyOzShZopU/D+6U84/+//jlhe1+Hpp8vx\n+UymTPGxe3dSfNUJITpAUtQWcuOdEEIkt+o77sBKSyPjscdaVb5TJ5PFi8uorNSYOtVHXZ3cjCeE\nOLIkSYztS2WnnSYtxkIIkYxMn4/q22/Hu3o1rn/+s1Xb9OsX5plnyvn8cxczZ2bLzXhCiCNKipvv\nNm92kZ5u0rVrJNahCNFqlmVRX1+PaZpohxmfNVaKi4sJBAKxDqPV2jpey7JwOBykpKTE5edzPKq5\n5RbSFi0i45FHKFu6tFXbjB4dYM6cKn7960x69Qpz111V7RylECKRtSoxNgxjLPAEoAMLlVIPHfD7\nWcDNQBjYC0xVSv27jWM9pM2b7f7F8t0lEkl9fT0ulwunMz7PT51OJ7quxzqMVmuPeMPhMPX19Xi9\n3jbdrzg2Vno61dOnkzVvHu6PPyZ47rmt2u5HP6pmyxYnjz+eQV5ehOuuk5EqhBAtO2JXCsMwdOAZ\n4BKgLzDZMIy+BxT7H2CAUup0YAXwSFsHeiiWZY9hLP2LRaIxTTNuk2JhczqdmGbrpxcW7a/mxhuJ\n5OWR+dBDtLZvhKbBQw9VcOGF9dxzTzazZ2dRV9fOgQohElJr+hgPArYqpQqVUkFgGXBl0wJKqXVK\nqYZT8E+Brm0b5qHt2uVg3z4Hp50mibFILHJ5PjHI5xRnvF6qZs7EvXEj3pUrW72ZxwMvvVTGnXdW\nsXRpGpdf3olvv02cKyJCiI7RmsT4JKCoyfMd0XWHMg14+/sEdTQabryTodqEODplZWWMGjWKUaNG\nceaZZ3LOOec0Pg8Gg63ax8yZM9m6dethyyxevJiVR5HACHEktZMnEzznHLLmzsWxa1ert3M64d57\nq3jppVJ27dK59NJOrFiRFPegCyHaSGuu47bUXNLi9SvDMK4HBgDDDvH7W4FbAZRS+P3+wwfndB6x\nzPbtdqV23nmZ5OQctmiHaE3M8SbRYk60eKHlmIuLi2PalaJz586sW7cOgEcffZS0tDT+4z/+o1kZ\nXdcbb0JryVNPPXXE17n55pu/f7Ct1B7vp8fjSbjjLenpOuXz59Np9GiyZ8+m7KWXOJqbTEaMCLB2\n7V5uvz2H665zc8UVOTzwwD78fuk2I8TxrjXfIjuA/CbPuwI7DyxkGMZI4D+BYUqpFm8NV0o9Dzwf\nfWqVlJQc9oX9fj9HKrNxYzZdumhEIiUcoWiHaE3M8SbRYk60eKHlmAOBQNzc3GaaJqZpEg6H2bZt\nG9OmTWPw4MFs3LiRF198kd/+9rd88cUX1NfXc8UVVzBz5kwAxo8fz69+9StOPfVUCgoKuOGGG3j/\n/ffxer288MIL+P1+Hn74YXw+H7fccgvjx49n0KBB/O1vf6OyspLHH3+cgQMHUltby4wZM9i2bRu9\ne/dm27ZtPProo/Tv379ZnI899hjvv/8+9fX1DBw4kIceeghN0/j222+57777KCsrQ9d1Fi5cSH5+\nPk8++SSrVq1C0zRGjhzJvffee9TvTSAQOOizO/HEE4/9zRZtInLyyVTefz/Z//mfpL78MrU33HBU\n2590UoSVK0tYvLgzv/lNCh9+6GHevH1cdVWd3MgtxHGsNYnx34FehmH8APgOmARc27SAYRhnAc8B\nY5VSe9o8ysPYvNkl/YtFwvvZzzL5+uu2nbq2b98Q8+ZVHtO233zzDU888QQPPvggAPfddx85OTmE\nw2EmTJjAuHHj6N27d7NtKisrGTJkCHPmzOEXv/gFy5YtY/r06Qft27Is3nrrLdauXcv8+fN55ZVX\nWLRoEZ06dWLBggV89dVXjB07tsW4pk2bxt13341lWdxxxx2sW7eOESNGcMcddzB79mwuvvhi6uvr\nsSyLtWvXsm7dOt588028Xi/l5eXH9F6I+FV7441416whc948AhdeSKR796Pa3uWC++4zGTasjLvv\nzubHP87h9de9/OIX+zjlFBn+U4jj0RE7VymlwsB04B1gk71KfWUYxjzDMK6IFnsUSAdeNQzjn4Zh\n/LndIm4iFIKtW50yIoUQbax79+6cddZZjc9XrVrFmDFjGDt2LFu2bOGbb745aJuUlBRGjBgBwOmn\nn05RUdFBZQAuueQSAAoKChrLfPbZZ1x5pX1Pb79+/ejTp0+L23700UeMGzeOUaNG8emnn/LNN99Q\nUVFBWVkZY8aMaYzD6/Xy0UcfMWnSpMah1nLioa+VaFsOB+X/9V+g62TPnAmRY0tme/cO89prJTzw\nwD4+/dTNhReewI03+vjwQ49MCiLEcaZVHfKUUquB1Qes+1mTn0e2cVytUljoJBjU5MY7kfCOtWW3\nvaSmpjb+XFhYyMKFC3nrrbfIysrizjvvbHEiDbfb3fizrutEDpGkNJRrWsZqRfZRV1fH/fffz5o1\na+jSpQsPP/ww9fX1QMsjR7RmnyLxmSedxL5588iZOZP0Z5+luoWrFK2h6zB1ag2XX17HSy+lsmRJ\nGpMn59KnT4ibbqrh0kvr6dRJ+iALkewS+nbchkvP0mIsRPuprq4mPT2djIwMiouL+eCDD9r8NQYN\nGsQbb7wBwKZNm1pska6rq8PhcODz+aiurmb1avtcPTs7G5/PxzvvvAPYE6fU1dVx4YUXsmzZMuqi\nA9ZKV4rkVTdhAnWXX07mgw/iff3177WvTp1MZs2qZv36YubPL8flspgzJ5uzzjqBK6/08/vfp7Ft\nW3zcGyCEaHsJnRivW+chO9ukTx9pMRaivRQUFNCrVy9GjBjB7NmzGThwYJu/xtSpU9m9ezcjR47k\nueeeo0+fPmRmZjYr4/P5mDBhAiNGjGDatGnNuno89dRT/P73v2fkyJH88Ic/pLS0lFGjRjF8+HAu\nvfRSRo0axYIFC9o8bhEnNI3y+fMJDB1K9owZeNrg5M3jgQkT6lizpoR3393DXXdVEQjAAw9kcf75\nJ3D55X5WrPASvWghhEgSWgwvN1o7dx40uEUzhxt9IByGM87I4+KL63nyyYr2iO+YJMuICfEs0eKF\nlmOura1t1mUh3jidTsLhjjnpDIfDhMNhUlJSKCws5Nprr+Wjjz46quHX2ivelj6n6KgUx9vYBUes\nsyG2/59aZSX+q69G/9e/KF2+nNDZZx9xm6ONd8cOnTffTOGVV9IoLHSSkxNh0qQ6Jk2qoWfPjrlh\nL1nqwHiWaPFC4sXc0fG2tt5O2PloN2xwU1HhYPRoOV0XItHV1NQwceLExsT24YcflumyxVGzMjMp\nfeUV/OPH47vxRkpfe41wr15t+hpdu0a4/fYabrutho8+crNkSRrPP5/Gs8+m0717mGHDAgwfHuDc\ncwNkZEg/dyESTcJ+87z7bgput8Xw4S0OmSyESCBZWVmsWbMm1mGIJGB27kzp0qV2cnzttZS+9hqR\nrl3b/HU0DS64IMgFFwTZtcvBO++k8MEHKaxY4WXJkjR03SI/P8IPfhCmR48wPXpE6N07zNlnB0lP\nl4RZiHiVsInx2rUpnHtuQCoYIYQQzUR69KD05ZfxGwa5EydSsmIFZpcu7fZ6XbqYTJlSy5QptQSD\nsHGjm48+8vDtt07+9S+dDRtSqaqyb+nRdYt+/UIMGhRk4MAg+fkROnWK0KmTiatthzIXQhyDhEyM\nt27VKSx0Mm1adaxDEUIIEYfC/ftT+vLL5E6eTO7EiZT+6U+YnTq1++u63TB0aJChQ4ON6ywLysoc\nfPmli/Xr3Xz2mZuXX05j4cL0Ztv6fBF69IgwcKCdNA8YEJQh4oToYAmZGL/7bgoAo0ZJNwohhBAt\nC519NmVLluC77jpyJ0+mRCksn6/D49A0yM01GTYswLBh9vdWIGDP3Lp7t86ePQ5KShwUF+ts3uxk\n8eI0nnvOTpq7dQuTnx8hLy9Cly7248knazgcbnJyTLKzLVJTTWpqHOzbp1FZ6aCy0kHnzhH69Qvh\n8XT4nytEQkvIxHjt2hT69Qtx0kkyZacQQohDCw4eTNkLL5B7003kXnstpX/8I1YczILo8cAZZ4Q4\n44yDx+EPBOCLL1xs2ODmn/90s3Onzqefuiku1gmHG26q97fiNSz69w9x9tlBCgpC+Hwm2dkmWVl2\nQt3wWoGARjCooesW3bpFpEuHOK4lXGJcWupgwwY3M2ZINwohvo9rrrmG6dOnM3z48MZ1CxYsoLCw\nkAcffPCQ2/Xq1YstW7awe/du5s6d2+L4wNdccw1z587ljDPOOOR+FixYwPXXX984ZfMNN9zA008/\nTVZW1rH/UUK0IHjBBZQtWIBv2jTyBgyg/qKLqL/sMupHjgT/kRPMjubxwIABIQYMCAE1jetN0/4O\nNM1ctm3bR0WFg4oKjepqBxkZJpmZFpmZ9uP27Tr/+Iebf/zDxUsvpVFf37rRBZ1Oi+7dw/TsGeaU\nU8J07rw/mc7JscjJMenUKUJmpkULE04KkfASLjF+7z0PpqnJMG1CfE9XXnklq1atapYYr1q1irlz\n57Zq+7y8vO81acbChQu5+uqrGxPjl1566Zj3JcSRBC6+mJI33sC7fDne1avxvv02lseDdfHFpJ1z\nDsGBAwkVFNidhOOUw2HPzOf3W5xwQvCwZQsKQowbZ39PhkLw7387KS/X2LfP0bgAuN0WHo+F220R\nCGgUFjr59lsnW7c6ef/9FEKhlrNft9vC77dvGvT5THJy9i9ZWVZjop6RYZKRYXHCCRpVVTput518\np6VZZGRIci3iT8Ilxu++m0JeXoSCApkGWojvY9y4cTzyyCMEAgE8Hg9FRUUUFxczaNAgampqmDp1\nKhUVFYTDYe655x7GjBnTbPuioiJuuukm3n//ferq6pg1axZbtmyhZ8+e1DeZDuzee+/lf//3f6mv\nr2fcuHHcfffd/OEPf6C4uJgJEyaQk5PDihUrGDx4MG+//TY+n4/nnnuO5cuXAzB58mRuueUWioqK\nuP766xk0aBAbNmwgLy+PRYsWNSbWDdauXcuTTz5JMBgkJyeHp59+mk6dOlFTU8P999/P559/jqZp\nzJw5k3HjxrFu3ToeeughIpEIPp8PpVT7v/kiJkIFBYQKCqicNw/3hg2kvPkmaevWkRWdXtxKSSF4\n1ll2uVNPJXzaaYR69YIDjrFE43JBz55HP/mNaUJlpRZtmbaXsjIHe/fafaL37tUpKXFQXu6gsNBJ\nebndv/nQTmj2zOm0yM7en1hnZdnJdFaWnVxnZ5vk5pr4fBH8fjvBrqran9xXVGhkZVn07Bmme/dw\nPJ/TiASSUIlxfT188IGHq6+uk7NMkVQyf/YzXF9/3ab7DPXtS+W8eYf8vc/n48wzz+SDDz5gzJgx\nrFq1iiuuuAJN0/B4PCxevBiv10tZWRmXX345o0ePRjvEP96SJUvwer385S9/4euvv2bs2LGNv/vp\nT39KTk4OkUiEiRMn8vXXXzNt2jSef/55Xn31VXwH3Az1+eefo5TizTffxLIsLrvsMoYOHUpWVhbb\ntm3jmWee4dFHH+W2225j9erVXH311c22HzRoEG+88QaaprF06VJ+97vf8fOf/5z58+eTkZHBe++9\nB0BFRQWlpaXMnj2blStX0q1bN8rLy4/17RaJxOEgOGgQwUGDcPv9lH39Ne6//71xSVuyBC16cmc5\nHETy8uyWZIcDy+kEXcfMzSVywglEunQhkpeHlZGBVldnL7W1aIEAkc6dieTnE+nWjUh+PlaCJdgO\nB2RnW2RnR4DW3dMTChG9AVCjqspBVZX9mJKSSVlZFaEQhEIaNTUa5eV2ol1ebi87djiprLQT34bh\n7VpL1y26d49w8slhTjwx0mzJzDTxeCy8XkhJsXA4LGprNerqHNTUaNTVabhcdgt2erpJerpFHHRD\nFzGSUInxxx97qK2V2e6EaCvjx49n1apVjYnx448/DoBlWfzmN7/hk08+QdM0du/ezd69e+ncuXOL\n+1m/fj1Tp04FoG/fvpx22mmNv3vjjTd45ZVXiEQiFBcXs2XLFvr27XvImD777DPGjh3bOA3zJZdc\nwvr16xk9ejT5+fn0798fgNNPP52ioqKDtt+1axc/+tGP2LNnD8FgkG7dugHw17/+ld/97neN5bKz\ns1m7di1DhgxpLJMj34bHJbNzZ+rHjaN+3Dh7RSSCvm0brs2bcW3ejF5UBKaJFg5DdNFLSnB/+il6\ncbG9/gCWpqFZzcfZN7OzMbOyMDMzsTIzMdPT0QIBHNXVaDU1aDU1oOtE/H7MhsXnw3K57CxV17F0\nHUdqKuk1NfY4cJYFmobl9WKmpWF5vVipqXa89fX7l3AYS9fB5bKT+4Y77EwTIhF7P7qO6fNh5uba\ni88HoZAdX2UljqoqME37pMDvx8rO5sBWKpfLHoEjNxeaJtN+fzolJXWt/kwiEdi3z0Fp6f6lqkoj\nPd1uSbb7PVuUlzvYutXu+rFli5Nt25yNM+N+X5rWBZfLbtl2u+2kOiXFwuu1SE21k+js7P19rzMz\nzYMS/kBAw+22cLnA5bIfU1Ls7isNj5YFNTUOamvtE4ZgUCM728TvN/H7I+Tm2mNcN71RMhSCYLD5\nzxkZOpaVgde7f99Op4Wug9MJDocV/cg1IhH7o7esxkMLhwM0bf8xa1n2Z+tw2H9vw+L1WoTD+2MJ\nBOz97X/f7CU11SItzSQtzSI93cLlav7/4HBAcbEjug/7b4hE7H01xAc0/j0Nr52WZrXr1YGESozX\nrk0hNdXk3HNlmDaRXA7Xstuexo4dyy9/+Uu++OIL6uvrKSgoAGDlypWUlJTw9ttv43K5GDx4MIHA\n4f/vWmpN3r59O8899xxvvfUW2dnZ/OQnP2nWzaIllnXoSXs8Tcae0nW9xX3NnTuXW2+9ldGjR/Px\nxx83S/ZbivFQreDHA8MwxgJPADqwUCn1UIxDig+6TqRnTyI9e1J/2WWHL2uaOEpL0aqqsFJT9yem\nTieOvXvRt2/HWVSEvn07+p49doJZWYlWWYlz+3aslBSstDQ70UxNRQuHcZSU4CwsxLF+PY7y8oMS\nbIDMdvrTj4bldGLm5tp/d0qK3Wc7+j+qBQL7k/JAAEdaGv7UVKy0NKz0dDvZj0TQGjKgcBitaZIe\niXBC099F11sej/1a0QVNY3gwiGY3RUOmhXViOqHUTKr0LCqsLAJhJ2bIJBKMYIZMLNNOUJ3RRXdp\nRCwHwZCDQMh+tPAQCITs8wZTw4xAKKIRDDsJhHQCpTr1u3SCtSaBOotgwAJMUtHIcmv0SdVI8YLD\nqREynQRNnVDESTCiEwprhMIa4bBGOAQuLUJXV4B0dz3pznpcWpjqgJuKOg91EQ+FuAnjxMRBBJ0I\nOiYOLOy6y6lb6LpFTfS9ChKmlhB6C638Flrjtg37C+AhiLvx0UWIFOobFxehxtdseDzwZwsNByYa\nFhrWQa/Z8HjgBzexNAAACf5JREFU0lC2pW0algOdMqYrty3q/b2P30NJmMTYsuAvf0lh+PAAKSmx\njkaI5JCWlsbQoUOZNWsW48ePb1xfVVWF3+/H5XLxt7/9jR07dhx2P4MHD+a1117jvPPOY/PmzWza\ntKlxP16vl8zMTPbu3cu6desYOnQoAOnp6VRXVx/UlWLIkCHMnDmT6dOnY1kWa9as4cknn2z131RZ\nWUleXh4Ar776auP6YcOG8cILLzAvehJSUVHBOeecw5w5c9i+fXtjV4rjpdXYMAwdeAYYBewA/m4Y\nxp+VUm3bpyfZORz2xCEtTB5idu6M2bkzoQEDjn3/lrW/VTcSQTNNcnNzKS0rs1uKAQ3sLhw1NXY3\njpoau9tH0wTS7bZbu0Mhu4U7FLKb9RyO/U2GwSCOsjIcZWXopaU4ysqwXC67hTs9HSszEwvQy8pw\n7N2Lo6TEPimoq7MT4WgyjGVh5uTsf22XC49lYZaWotXUoO/cab9+tAU82pxp/9wQj9uN2fR3Tido\nmv0adXX2ycWePfZbZDfHYrndoGk4iovxVm0ltbKSLlVV9nun6/brNey/obXdiqZeDe9ztBlVM02s\nA06aWzpBaVEwulQcxeccAaLn+Zau2ycCR7PtcTR67ZbIFODX7bb/hEmMNQ3+/Oe91NZ+/8sjQoj9\nxo8fz80338yzzz7buO6qq65iypQpXHLJJfTr14+ePXsedh833ngjs2bNYuTIkfTt25czzzwTgH79\n+tG/f38uuugiunXrxsCBAxu3ue6667j++uvp3LkzK1asaFxfUFDAhAkTGBe9rD158mT69+/fYreJ\nltx1113cdttt5OXlcfbZZzduN2PGDObMmcOIESNwOBzMmjWLSy+9lEceeYSbb74Z0zTx+/0sW7as\ndW9c4hsEbFVKFQIYhrEMuBKQxDieaFpjUgfY7Wrp6VhNrpZYYPdfboPJSyInn/y999ESv99PWUlJ\nu+y7Pfj9fkoOjLdp8hyJoFmWnWjv74dgl2soY5r7W8GbnNgcuE9L18HtthN7l8vej2XtP4kJBJrt\nR2vY/wH7yfH7Kauqsk8SGo6ZA6+IHXgCEInY+w+F0IJBCAbt7Rta5j0e++Skyd/c2M/BNO3nDftr\neA+0/e2/WtPXPWDJyc6mvKJif4wNjweWPeBvyMjIoD3ng9QOd9mynVk7d+48bIEWD8w4JzG3v0SL\nF1qOuba2trEfbTxyOp2EW+g7Ga/aK96WPqcTTzwRaOEaXwIxDOMaYKxS6ubo8xuAwUqp6YfY5Ih1\nNiTe/2eixQsSc0dItHgh8WLu6HhbW28nTIuxEEKINtXSF0SzlhLDMG4FbgVQSuFvxWQYTqezVeXi\nRaLFCxJzR0i0eCHxYo7XeCUxFkKI49MOIL/J865AsyZhpdTzwPPRp1ZrWnek1ar9ScztL9HihcSL\nOUYtxkckibEQQhyf/g70MgzjB8B3wCTg2tiGJIQQsSV3sgkRIzHs3y+OQrJ+TkqpMDAdeAfYZK9S\nX8U2KiGEiC1pMRYiRhwOB+FwGKdT/g3jVTgcxuFI3vYDpdRqYHWs4xBCiHgh38hCxEhKSgr19fUE\nAoG4nGTC4/EccVKPeNLW8VqWhcPhIEUGThdCiOOGJMZCxIimaXi93liHcUhyI4cQQojjTfJeIxRC\nCCGEEOIoSGIshBBCCCEEkhgLIYQQQggBxHhK6Fi9sBBCtIH4u2OyfUmdLYRIdEest2PZYqwdaTEM\nY2NrysXTIjFLvBLzcRPv8SZeP4dEO24k5gRYEi3eRIw5RvEekXSlEEIIIYQQAkmMhRBCCCGEAOI/\nMX4+1gEcA4m5/SVavCAxd4REizdZJdrnkGjxgsTcERItXki8mOMy3ljefCeEEEIIIUTciPcWYyGE\nEEIIITpE3E4JbRjGWOAJQAcWKqUeinFIBzEMYxFwGbBHKdU/us4HLAd6AP8CDKVUeaxibMowjHxg\nCZAHmMDzSqkn4jzmFOBDwIN9vK5QSv3cMIwfAMsAH/AP4AalVDB2kTZnGIYObAC+U0pdlgDx/guo\nAiJAWCk1IJ6PCwDDMLKBhUB/7KHEpgL/RxzHnMykzm57Umd3HKmz21+i1Nlx2WIcPUCfAS4B+gKT\nDcPoG9uoWrQYGHvAunuB95RSvYD3os/jRRi4Syl1GjAEuCP6vsZzzAFghFLqDOBMYKxhGEOAh4Hf\nRmMuB6bFMMaWzAA2NXke7/ECXKSUOlMpNSD6PJ6PC7CTsDVKqVOBM7Df73iPOSlJnd1upM7uOFJn\nt7+EqLPjMjEGBgFblVKF0TO0ZcCVMY7pIEqpD4GyA1ZfCbwY/flFYHyHBnUYSqldSql/RH+uwj4o\nTyK+Y7aUUtXRp67oYgEjgBXR9XEVs2EYXYFx2GfGGIahEcfxHkbcHheGYWQCFwJ/AFBKBZVSFcRx\nzElO6ux2IHV2x5A6u/0lUp0dr4nxSUBRk+c7ousSwQlKqV1gV2pA5xjH0yLDMHoAZwHrifOYDcPQ\nDcP4J7AHeBf4FqhQSoWjReLt+JgP3IN96RMgl/iOF+wvrrWGYWw0DOPW6Lp4Pi5OBvYCLxiG8T+G\nYSw0DCON+I45mUmd3c6kzm5XUme3v4Sps+M1MW5pdhIZPqONGIaRDvwJ+IlSqjLW8RyJUiqilDoT\n6IrdMnVaC8Xi4vgwDKOh/+LGJqsT4Xg+Tyl1Nval8DsMw7gw1gEdgRM4G3hWKXUWUEMcXII7jiXC\nMZ6wpM5uP1Jnd5iEqbPjNTHeAeQ3ed4V2BmjWI5WsWEYXQCij3tiHE8zhmG4sCvYV5RSK6Or4zrm\nBtHLLh9g97XLNgyj4ebReDo+zgOuiN4YsQz7ctx84jdeAJRSO6OPe4DXsL/M4vm42AHsUEqtjz5f\ngV3pxnPMyUzq7HYidXa7kzq7YyRMnR2vifHfgV6GYfzAMAw3MAn4c4xjaq0/AzdFf74JWBXDWJqJ\n9pv6A7BJKfV4k1/Fc8ydoneyYhiGFxiJ3c9uHXBNtFjcxKyUuk8p1VUp1QP7uH1fKXUdcRovgGEY\naYZhZDT8DIwGviSOjwul1G6gyDCMPtFVFwNfE8cxJzmps9uB1NntT+rsjpFIdXbcTvBhGMal2Gdt\nOrBIKfXrGId0EMMw/ggMB/xAMfBz4HVAAd2A7cAEpdSBN3vEhGEY5wN/Bb5gf1+qOdh91uI15tOx\nO+Tr2CdySik1zzCMk9k/lM7/ANcrpQKxi/RghmEMB+6ODv0Tt/FGY3st+tQJLFVK/dowjFzi9LgA\nMAzjTOybZdxAIfD/iB4jxGnMyUzq7LYndXbHkjq7fSVKnR23ibEQQgghhBAdKV67UgghhBBCCNGh\nJDEWQgghhBACSYyFEEIIIYQAJDEWQgghhBACkMRYCCGEEEIIQBJjIYQQQgghAEmMhRBCCCGEACQx\nFkIIIYQQAoD/D3/bztVOwW3nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15e131fa080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1149    0    0    3    1    1    3    0    1    2]\n",
      " [   0 1028    0    0    0    0    1    1    1    0]\n",
      " [   2    1  997    2    0    0    2   11    2    0]\n",
      " [   2    0    0 1065    0    0    2    5    2    0]\n",
      " [   0    0    0    1 1079    3    4    1    8    0]\n",
      " [   1    3    0    0    1  916    8    2    0    4]\n",
      " [   0    0    2    1    2    2 1033    6    2    2]\n",
      " [   0    3    7    2    0    2    4 1030    0    0]\n",
      " [   7    1    0   10    1    0    4    0 1057    0]\n",
      " [   0    2    2    0    0    2    2    0    1  998]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1160\n",
      "           1       0.99      1.00      0.99      1031\n",
      "           2       0.99      0.98      0.98      1017\n",
      "           3       0.98      0.99      0.99      1076\n",
      "           4       1.00      0.98      0.99      1096\n",
      "           5       0.99      0.98      0.98       935\n",
      "           6       0.97      0.98      0.98      1050\n",
      "           7       0.98      0.98      0.98      1048\n",
      "           8       0.98      0.98      0.98      1080\n",
      "           9       0.99      0.99      0.99      1007\n",
      "\n",
      "    accuracy                           0.99     10500\n",
      "   macro avg       0.99      0.99      0.99     10500\n",
      "weighted avg       0.99      0.99      0.99     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values/255 ### 0.99014\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 28, 28, 1)\n",
      "(10500, 28, 28, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_21 (Spatia (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_22 (Spatia (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 7, 7, 192)         221376    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 3, 3, 192)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1728)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1600)              2766400   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 800)               1280800   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 5,059,026\n",
      "Trainable params: 5,059,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(31500, 28, 28, 1)\n",
      "Epoch 1/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 2.2616 - acc: 0.1416Epoch 00001: val_acc improved from -inf to 0.36867, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 19s 153ms/step - loss: 2.2598 - acc: 0.1425 - val_loss: 1.7834 - val_acc: 0.3687\n",
      "Epoch 2/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 1.5050 - acc: 0.4558Epoch 00002: val_acc improved from 0.36867 to 0.90305, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 17s 138ms/step - loss: 1.5012 - acc: 0.4574 - val_loss: 0.4196 - val_acc: 0.9030\n",
      "Epoch 3/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.7965 - acc: 0.7396Epoch 00003: val_acc improved from 0.90305 to 0.94476, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 134ms/step - loss: 0.7960 - acc: 0.7397 - val_loss: 0.1879 - val_acc: 0.9448\n",
      "Epoch 4/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.5637 - acc: 0.8234Epoch 00004: val_acc improved from 0.94476 to 0.96048, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.5628 - acc: 0.8238 - val_loss: 0.1332 - val_acc: 0.9605\n",
      "Epoch 5/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.4501 - acc: 0.8606Epoch 00005: val_acc improved from 0.96048 to 0.96952, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.4495 - acc: 0.8606 - val_loss: 0.1014 - val_acc: 0.9695\n",
      "Epoch 6/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.3841 - acc: 0.8834Epoch 00006: val_acc improved from 0.96952 to 0.97171, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.3835 - acc: 0.8835 - val_loss: 0.0953 - val_acc: 0.9717\n",
      "Epoch 7/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.3545 - acc: 0.8947Epoch 00007: val_acc improved from 0.97171 to 0.97352, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.3544 - acc: 0.8945 - val_loss: 0.0885 - val_acc: 0.9735\n",
      "Epoch 8/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.9080Epoch 00008: val_acc improved from 0.97352 to 0.97619, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.3113 - acc: 0.9082 - val_loss: 0.0781 - val_acc: 0.9762\n",
      "Epoch 9/100\n",
      "121/123 [============================>.] - ETA: 0s - loss: 0.2807 - acc: 0.9148Epoch 00009: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.2818 - acc: 0.9148 - val_loss: 0.0751 - val_acc: 0.9759\n",
      "Epoch 10/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.2660 - acc: 0.9193Epoch 00010: val_acc improved from 0.97619 to 0.98067, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.2657 - acc: 0.9194 - val_loss: 0.0658 - val_acc: 0.9807\n",
      "Epoch 11/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9300Epoch 00011: val_acc improved from 0.98067 to 0.98076, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.2366 - acc: 0.9300 - val_loss: 0.0660 - val_acc: 0.9808\n",
      "Epoch 12/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9328Epoch 00012: val_acc improved from 0.98076 to 0.98248, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.2268 - acc: 0.9329 - val_loss: 0.0576 - val_acc: 0.9825\n",
      "Epoch 13/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9369Epoch 00013: val_acc did not improve\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.2162 - acc: 0.9369 - val_loss: 0.0711 - val_acc: 0.9801\n",
      "Epoch 14/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9389Epoch 00014: val_acc improved from 0.98248 to 0.98457, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.2024 - acc: 0.9390 - val_loss: 0.0525 - val_acc: 0.9846\n",
      "Epoch 15/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.2059 - acc: 0.9383Epoch 00015: val_acc improved from 0.98457 to 0.98476, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.2060 - acc: 0.9382 - val_loss: 0.0511 - val_acc: 0.9848\n",
      "Epoch 16/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1867 - acc: 0.9435Epoch 00016: val_acc improved from 0.98476 to 0.98514, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1866 - acc: 0.9434 - val_loss: 0.0509 - val_acc: 0.9851\n",
      "Epoch 17/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9447Epoch 00017: val_acc improved from 0.98514 to 0.98733, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1799 - acc: 0.9446 - val_loss: 0.0468 - val_acc: 0.9873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "121/123 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9495Epoch 00018: val_acc improved from 0.98733 to 0.98790, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1743 - acc: 0.9492 - val_loss: 0.0422 - val_acc: 0.9879\n",
      "Epoch 19/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1669 - acc: 0.9480Epoch 00019: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1668 - acc: 0.9479 - val_loss: 0.0456 - val_acc: 0.9870\n",
      "Epoch 20/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9531Epoch 00020: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1599 - acc: 0.9530 - val_loss: 0.0427 - val_acc: 0.9878\n",
      "Epoch 21/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9527Epoch 00021: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1591 - acc: 0.9526 - val_loss: 0.0509 - val_acc: 0.9861\n",
      "Epoch 22/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1559 - acc: 0.9536Epoch 00022: val_acc did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 3.2999999166349884e-05.\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.1557 - acc: 0.9537 - val_loss: 0.0544 - val_acc: 0.9845\n",
      "Epoch 23/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9579Epoch 00023: val_acc improved from 0.98790 to 0.98876, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.1446 - acc: 0.9578 - val_loss: 0.0389 - val_acc: 0.9888\n",
      "Epoch 24/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9594Epoch 00024: val_acc improved from 0.98876 to 0.98933, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1387 - acc: 0.9594 - val_loss: 0.0389 - val_acc: 0.9893\n",
      "Epoch 25/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.9612Epoch 00025: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1303 - acc: 0.9611 - val_loss: 0.0384 - val_acc: 0.9889\n",
      "Epoch 26/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1283 - acc: 0.9627Epoch 00026: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1283 - acc: 0.9625 - val_loss: 0.0371 - val_acc: 0.9892\n",
      "Epoch 27/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.9628Epoch 00027: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1249 - acc: 0.9627 - val_loss: 0.0397 - val_acc: 0.9889\n",
      "Epoch 28/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1198 - acc: 0.9653Epoch 00028: val_acc improved from 0.98933 to 0.98962, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1197 - acc: 0.9654 - val_loss: 0.0370 - val_acc: 0.9896\n",
      "Epoch 29/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9640Epoch 00029: val_acc improved from 0.98962 to 0.98971, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1190 - acc: 0.9640 - val_loss: 0.0380 - val_acc: 0.9897\n",
      "Epoch 30/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9645Epoch 00030: val_acc did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 1.0890000085055363e-05.\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1215 - acc: 0.9645 - val_loss: 0.0380 - val_acc: 0.9892\n",
      "Epoch 31/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1251 - acc: 0.9636Epoch 00031: val_acc improved from 0.98971 to 0.98990, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1250 - acc: 0.9637 - val_loss: 0.0371 - val_acc: 0.9899\n",
      "Epoch 32/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1173 - acc: 0.9649Epoch 00032: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1170 - acc: 0.9650 - val_loss: 0.0371 - val_acc: 0.9899\n",
      "Epoch 33/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9645Epoch 00033: val_acc improved from 0.98990 to 0.99019, saving model to CNN+overkill+da/weights.hdf5\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1125 - acc: 0.9646 - val_loss: 0.0351 - val_acc: 0.9902\n",
      "Epoch 34/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9671Epoch 00034: val_acc did not improve\n",
      "123/123 [==============================] - 16s 129ms/step - loss: 0.1118 - acc: 0.9671 - val_loss: 0.0365 - val_acc: 0.9896\n",
      "Epoch 35/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1162 - acc: 0.9652Epoch 00035: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1161 - acc: 0.9651 - val_loss: 0.0371 - val_acc: 0.9900\n",
      "Epoch 36/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1135 - acc: 0.9657Epoch 00036: val_acc did not improve\n",
      "123/123 [==============================] - 16s 130ms/step - loss: 0.1135 - acc: 0.9658 - val_loss: 0.0357 - val_acc: 0.9900\n",
      "Epoch 37/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1095 - acc: 0.9672Epoch 00037: val_acc improved from 0.99019 to 0.99029, saving model to CNN+overkill+da/weights.hdf5\n",
      "\n",
      "Epoch 00037: reducing learning rate to 3.59370011210558e-06.\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1094 - acc: 0.9673 - val_loss: 0.0350 - val_acc: 0.9903\n",
      "Epoch 38/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1092 - acc: 0.9673Epoch 00038: val_acc did not improve\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1092 - acc: 0.9674 - val_loss: 0.0356 - val_acc: 0.9902\n",
      "Epoch 39/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9670Epoch 00039: val_acc did not improve\n",
      "123/123 [==============================] - 16s 132ms/step - loss: 0.1107 - acc: 0.9670 - val_loss: 0.0356 - val_acc: 0.9900\n",
      "Epoch 40/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9683Epoch 00040: val_acc did not improve\n",
      "\n",
      "Epoch 00040: reducing learning rate to 1.1859210189868463e-06.\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1076 - acc: 0.9684 - val_loss: 0.0360 - val_acc: 0.9899\n",
      "Epoch 41/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1089 - acc: 0.9678Epoch 00041: val_acc did not improve\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1086 - acc: 0.9678 - val_loss: 0.0356 - val_acc: 0.9902\n",
      "Epoch 42/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9690Epoch 00042: val_acc did not improve\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1089 - acc: 0.9690 - val_loss: 0.0358 - val_acc: 0.9900\n",
      "Epoch 43/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9661Epoch 00043: val_acc did not improve\n",
      "\n",
      "Epoch 00043: reducing learning rate to 3.913539512723219e-07.\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1104 - acc: 0.9662 - val_loss: 0.0356 - val_acc: 0.9901\n",
      "Epoch 44/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9660Epoch 00044: val_acc did not improve\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1100 - acc: 0.9661 - val_loss: 0.0358 - val_acc: 0.9899\n",
      "Epoch 45/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9676Epoch 00045: val_acc did not improve\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1074 - acc: 0.9676 - val_loss: 0.0358 - val_acc: 0.9900\n",
      "Epoch 46/100\n",
      "122/123 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9664Epoch 00046: val_acc did not improve\n",
      "\n",
      "Epoch 00046: reducing learning rate to 1.2914680354469966e-07.\n",
      "123/123 [==============================] - 16s 131ms/step - loss: 0.1141 - acc: 0.9663 - val_loss: 0.0358 - val_acc: 0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00046: early stopping\n"
     ]
    }
   ],
   "source": [
    "### CNN OVERKILL  w/ data augmentation\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout, SpatialDropout2D\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=12,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "filters = 400\n",
    "kernel_sizes = [3,3,3]\n",
    "\n",
    "x_train = x_train.reshape(-1,dim,dim,1)\n",
    "x_test = x_test.reshape(-1,dim,dim,1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "datagen.fit(x_train)\n",
    "\n",
    "inputs = Input(shape=(dim,dim,1))\n",
    "#reshape = Reshape((dim, dim, 1))(inputs)\n",
    "\n",
    "conv1 = Conv2D(filters = 32, kernel_size = kernel_sizes[0], padding='same')(inputs)\n",
    "conv2 = Conv2D(filters = 64, kernel_size = kernel_sizes[0], padding='same')(conv1)\n",
    "conv3 = Conv2D(filters = 128, kernel_size = kernel_sizes[0], padding='same')(conv2)\n",
    "max1 = MaxPooling2D()(conv3)\n",
    "spatial = SpatialDropout2D(0.4)(max1)\n",
    "conv4 = Conv2D(filters = 128, kernel_size = kernel_sizes[1], padding='same')(spatial)\n",
    "conv5 = Conv2D(filters = 128, kernel_size = kernel_sizes[1], padding='same')(conv4)\n",
    "max2 = MaxPooling2D()(conv5)\n",
    "spatial2 = SpatialDropout2D(0.2)(max2)\n",
    "conv6 = Conv2D(filters = 192, kernel_size = kernel_sizes[2], padding='same')(spatial2)\n",
    "#conv6 = Conv2D(filters = 128, kernel_size = kernel_sizes[2])(conv5)\n",
    "max3 = MaxPooling2D()(conv6)\n",
    "flat1=Flatten()(max3)\n",
    "dense1 = Dense(units=1600, activation='relu')(flat1)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units=800, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(units=400, activation='relu')(drop2)\n",
    "drop3 = Dropout(0.5)(dense3)\n",
    "dense4 = Dense(units=200, activation='relu')(drop3)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense4)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'CNN+overkill+da'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_train,y_train, batch_size=BATCH_SIZE),\n",
    "                              epochs = EPOCHS, validation_data=(x_test, y_test),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // BATCH_SIZE\n",
    "                              , callbacks = [checkpoint, early_stopping, reduceLR], class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAFACAYAAAC/abrtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2316b8f0940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1140    0    0    3    0    0    0    0    2    0]\n",
      " [   0 1067    0    0    0    0    2    0    0    3]\n",
      " [   2    0 1035    1    0    0    0    4    0    2]\n",
      " [   1    0    6 1127    0    0    2    1    1    0]\n",
      " [   0    0    0    0 1059    4    1    1    7    0]\n",
      " [   0    0    0    0    2  909    2    0    0    6]\n",
      " [   1    2    2    1    0    1  983    6    3    0]\n",
      " [   0    0    5    2    0    1    2 1032    0    0]\n",
      " [   3    2    0    7    0    0    4    0 1005    0]\n",
      " [   0    1    2    0    0    0    3    0    1 1041]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1145\n",
      "           1       1.00      1.00      1.00      1072\n",
      "           2       0.99      0.99      0.99      1044\n",
      "           3       0.99      0.99      0.99      1138\n",
      "           4       1.00      0.99      0.99      1072\n",
      "           5       0.99      0.99      0.99       919\n",
      "           6       0.98      0.98      0.98       999\n",
      "           7       0.99      0.99      0.99      1042\n",
      "           8       0.99      0.98      0.99      1021\n",
      "           9       0.99      0.99      0.99      1048\n",
      "\n",
      "    accuracy                           0.99     10500\n",
      "   macro avg       0.99      0.99      0.99     10500\n",
      "weighted avg       0.99      0.99      0.99     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values/255 ### 0.99385\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim, 1)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
