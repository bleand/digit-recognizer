{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "dim = int(sqrt(len(df.columns)-1))\n",
    "print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_sample = 8\n",
    "sample = df.drop(columns='label').iloc[[ix_sample]].values.reshape(dim,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c902bd8908>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBThB4Ii/EBQ/w9n8S0vWL63KQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c9009a1ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "im = Image.fromarray(np.uint8(cm.gist_earth(sample)*255))\n",
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data, dtype=np.int16).reshape(-1)\n",
    "    return np.eye(nb_classes,dtype=np.int8)[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels = df['label'].values\n",
    "labels, levels = pd.factorize(labels)\n",
    "classes = len(np.unique(labels))\n",
    "y = indices_to_one_hot(labels, classes)\n",
    "data = df.drop(columns=['label']).values\n",
    "x_train, x_test, y_train, y_test = train_test_split(data,y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31500\n",
      "10500\n",
      "31500\n",
      "10500\n",
      "classes: 10\n",
      "[1 0 4 7 3 5 8 9 2 6]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "print(\"classes:\", classes)\n",
    "print(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple NN\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(input_shape,))\n",
    "dense1 = Dense(units=1000, activation='relu')(inputs)\n",
    "dense2 = Dense(units=1000, activation='sigmoid')(dense1)\n",
    "dense3 = Dense(units=1000, activation='sigmoid')(dense2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'NN'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some visual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, x_test):\n",
    "    import csv\n",
    "    csv_file = open(path+'/submission.csv', 'w', newline='', encoding='utf-8')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['ImageId','Label'])\n",
    "    pred = model.predict(x_test)\n",
    "    #ix = 155\n",
    "    #im = Image.fromarray(np.uint8(cm.gist_earth(df.values[ix].reshape(dim,dim))*255))\n",
    "    #imshow(im)\n",
    "    #print(levels[np.argmax(pred[ix])])\n",
    "    for ix, p in enumerate(pred):\n",
    "        writer.writerow([ix+1, levels[np.argmax(p)]])\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv') ### 0.96785\n",
    "generate_submission(model, test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "### CNN\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "filters = 10\n",
    "kernel_size = 4\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_size)(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "flat1=Flatten()(max1)\n",
    "dense1 = Dense(units=1000, activation='relu')(flat1)\n",
    "dense2 = Dense(units=1000, activation='sigmoid')(dense1)\n",
    "dense3 = Dense(units=1000, activation='sigmoid')(dense2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 10)        170       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              1441000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 3,453,180\n",
      "Trainable params: 3,453,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 784)\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/100\n",
      "30720/31500 [============================>.] - ETA: 0s - loss: 0.7939 - acc: 0.8208Epoch 00001: val_acc improved from -inf to 0.92067, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 5s 155us/step - loss: 0.7810 - acc: 0.8237 - val_loss: 0.3002 - val_acc: 0.9207\n",
      "Epoch 2/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9417Epoch 00002: val_acc improved from 0.92067 to 0.94438, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.2175 - acc: 0.9416 - val_loss: 0.1914 - val_acc: 0.9444\n",
      "Epoch 3/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.9621Epoch 00003: val_acc improved from 0.94438 to 0.95495, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.1420 - acc: 0.9622 - val_loss: 0.1521 - val_acc: 0.9550\n",
      "Epoch 4/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9727Epoch 00004: val_acc improved from 0.95495 to 0.96390, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 69us/step - loss: 0.1018 - acc: 0.9727 - val_loss: 0.1207 - val_acc: 0.9639\n",
      "Epoch 5/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0747 - acc: 0.9809Epoch 00005: val_acc improved from 0.96390 to 0.96952, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 69us/step - loss: 0.0745 - acc: 0.9808 - val_loss: 0.1072 - val_acc: 0.9695\n",
      "Epoch 6/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9868Epoch 00006: val_acc improved from 0.96952 to 0.97200, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.0535 - acc: 0.9868 - val_loss: 0.0931 - val_acc: 0.9720\n",
      "Epoch 7/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9923Epoch 00007: val_acc improved from 0.97200 to 0.97305, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.0365 - acc: 0.9923 - val_loss: 0.0861 - val_acc: 0.9730\n",
      "Epoch 8/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9952Epoch 00008: val_acc improved from 0.97305 to 0.97410, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 71us/step - loss: 0.0257 - acc: 0.9952 - val_loss: 0.0821 - val_acc: 0.9741\n",
      "Epoch 9/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9977Epoch 00009: val_acc improved from 0.97410 to 0.97705, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 71us/step - loss: 0.0170 - acc: 0.9977 - val_loss: 0.0769 - val_acc: 0.9770\n",
      "Epoch 10/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9989Epoch 00010: val_acc improved from 0.97705 to 0.97790, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 71us/step - loss: 0.0117 - acc: 0.9989 - val_loss: 0.0751 - val_acc: 0.9779\n",
      "Epoch 11/100\n",
      "30720/31500 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9993Epoch 00011: val_acc improved from 0.97790 to 0.97810, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.0083 - acc: 0.9994 - val_loss: 0.0743 - val_acc: 0.9781\n",
      "Epoch 12/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9996Epoch 00012: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.0063 - acc: 0.9995 - val_loss: 0.0750 - val_acc: 0.9774\n",
      "Epoch 13/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9996Epoch 00013: val_acc improved from 0.97810 to 0.97848, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.0054 - acc: 0.9997 - val_loss: 0.0748 - val_acc: 0.9785\n",
      "Epoch 14/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9997Epoch 00014: val_acc improved from 0.97848 to 0.97857, saving model to CNN/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 70us/step - loss: 0.0041 - acc: 0.9997 - val_loss: 0.0751 - val_acc: 0.9786\n",
      "Epoch 15/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9998Epoch 00015: val_acc did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 3.2999999166349884e-05.\n",
      "31500/31500 [==============================] - 2s 69us/step - loss: 0.0035 - acc: 0.9998 - val_loss: 0.0743 - val_acc: 0.9785\n",
      "Epoch 16/100\n",
      "30720/31500 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9999Epoch 00016: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 68us/step - loss: 0.0029 - acc: 0.9999 - val_loss: 0.0745 - val_acc: 0.9781\n",
      "Epoch 17/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9999Epoch 00017: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 68us/step - loss: 0.0027 - acc: 0.9999 - val_loss: 0.0746 - val_acc: 0.9784\n",
      "Epoch 18/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9999Epoch 00018: val_acc did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 1.0890000085055363e-05.\n",
      "31500/31500 [==============================] - 2s 68us/step - loss: 0.0026 - acc: 0.9999 - val_loss: 0.0746 - val_acc: 0.9786\n",
      "Epoch 19/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9999Epoch 00019: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 68us/step - loss: 0.0025 - acc: 0.9999 - val_loss: 0.0746 - val_acc: 0.9784\n",
      "Epoch 20/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9999Epoch 00020: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 68us/step - loss: 0.0024 - acc: 0.9999 - val_loss: 0.0747 - val_acc: 0.9784\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'CNN'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1126    0    0    4    1    0    1    1    3    0]\n",
      " [   0  956    2    0    0    2    3    1    0    5]\n",
      " [   2    0  983    2    0    1    1    5    3    5]\n",
      " [   2    1    4 1100    0    0    1    9    8    0]\n",
      " [   1    0    0    3 1078   12    9    4    7    0]\n",
      " [   1    4    3    1    5  937    1    5    0    6]\n",
      " [   4    1    2    1    5    5  965    4    3    5]\n",
      " [   2    2    8    5    3    4    6 1076    0    0]\n",
      " [   3    5    4    4    3    2    1    1 1032    1]\n",
      " [   2    4    2    0    0    1    2    0    1 1022]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1136\n",
      "           1       0.98      0.99      0.98       969\n",
      "           2       0.98      0.98      0.98      1002\n",
      "           3       0.98      0.98      0.98      1125\n",
      "           4       0.98      0.97      0.98      1114\n",
      "           5       0.97      0.97      0.97       963\n",
      "           6       0.97      0.97      0.97       995\n",
      "           7       0.97      0.97      0.97      1106\n",
      "           8       0.98      0.98      0.98      1056\n",
      "           9       0.98      0.99      0.98      1034\n",
      "\n",
      "    accuracy                           0.98     10500\n",
      "   macro avg       0.98      0.98      0.98     10500\n",
      "weighted avg       0.98      0.98      0.98     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values ### 0.97671\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN + dropout\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "filters = 10\n",
    "kernel_size = 4\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_size)(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "flat1=Flatten()(max1)\n",
    "dense1 = Dense(units=1000, activation='relu')(flat1)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units=1000, activation='sigmoid')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(units=1000, activation='sigmoid')(drop2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 25, 25, 10)        170       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 10)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1000)              1441000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 3,453,180\n",
      "Trainable params: 3,453,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 28, 28)\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 1.4332 - acc: 0.6121Epoch 00001: val_acc improved from -inf to 0.85152, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 3s 97us/step - loss: 1.4208 - acc: 0.6160 - val_loss: 0.5046 - val_acc: 0.8515\n",
      "Epoch 2/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.8685Epoch 00002: val_acc improved from 0.85152 to 0.90895, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.5091 - acc: 0.8685 - val_loss: 0.3001 - val_acc: 0.9090\n",
      "Epoch 3/100\n",
      "30720/31500 [============================>.] - ETA: 0s - loss: 0.3312 - acc: 0.9063Epoch 00003: val_acc improved from 0.90895 to 0.92886, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.3304 - acc: 0.9066 - val_loss: 0.2407 - val_acc: 0.9289\n",
      "Epoch 4/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9225Epoch 00004: val_acc improved from 0.92886 to 0.93933, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.2622 - acc: 0.9224 - val_loss: 0.2060 - val_acc: 0.9393\n",
      "Epoch 5/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9341Epoch 00005: val_acc improved from 0.93933 to 0.94457, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.2198 - acc: 0.9343 - val_loss: 0.1843 - val_acc: 0.9446\n",
      "Epoch 6/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.1869 - acc: 0.9442Epoch 00006: val_acc improved from 0.94457 to 0.95238, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.1869 - acc: 0.9443 - val_loss: 0.1629 - val_acc: 0.9524\n",
      "Epoch 7/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9503Epoch 00007: val_acc improved from 0.95238 to 0.95533, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.1645 - acc: 0.9504 - val_loss: 0.1518 - val_acc: 0.9553\n",
      "Epoch 8/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9549Epoch 00008: val_acc improved from 0.95533 to 0.95952, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.1488 - acc: 0.9551 - val_loss: 0.1365 - val_acc: 0.9595\n",
      "Epoch 9/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.1320 - acc: 0.9602Epoch 00009: val_acc improved from 0.95952 to 0.96162, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.1326 - acc: 0.9601 - val_loss: 0.1297 - val_acc: 0.9616\n",
      "Epoch 10/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9635Epoch 00010: val_acc improved from 0.96162 to 0.96419, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.1227 - acc: 0.9634 - val_loss: 0.1252 - val_acc: 0.9642\n",
      "Epoch 11/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.1076 - acc: 0.9675Epoch 00011: val_acc improved from 0.96419 to 0.96552, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.1074 - acc: 0.9677 - val_loss: 0.1183 - val_acc: 0.9655\n",
      "Epoch 12/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9689Epoch 00012: val_acc improved from 0.96552 to 0.96895, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0981 - acc: 0.9689 - val_loss: 0.1102 - val_acc: 0.9690\n",
      "Epoch 13/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9707Epoch 00013: val_acc improved from 0.96895 to 0.97038, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0941 - acc: 0.9709 - val_loss: 0.1054 - val_acc: 0.9704\n",
      "Epoch 14/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9748Epoch 00014: val_acc improved from 0.97038 to 0.97048, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0835 - acc: 0.9747 - val_loss: 0.1063 - val_acc: 0.9705\n",
      "Epoch 15/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9751Epoch 00015: val_acc improved from 0.97048 to 0.97248, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 74us/step - loss: 0.0824 - acc: 0.9751 - val_loss: 0.1018 - val_acc: 0.9725\n",
      "Epoch 16/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0749 - acc: 0.9772Epoch 00016: val_acc improved from 0.97248 to 0.97362, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0747 - acc: 0.9773 - val_loss: 0.0996 - val_acc: 0.9736\n",
      "Epoch 17/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9795Epoch 00017: val_acc improved from 0.97362 to 0.97381, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0664 - acc: 0.9795 - val_loss: 0.0963 - val_acc: 0.9738\n",
      "Epoch 18/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9806Epoch 00018: val_acc improved from 0.97381 to 0.97457, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0626 - acc: 0.9806 - val_loss: 0.0959 - val_acc: 0.9746\n",
      "Epoch 19/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9811Epoch 00019: val_acc improved from 0.97457 to 0.97552, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0607 - acc: 0.9810 - val_loss: 0.0948 - val_acc: 0.9755\n",
      "Epoch 20/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9819Epoch 00020: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0590 - acc: 0.9818 - val_loss: 0.0953 - val_acc: 0.9748\n",
      "Epoch 21/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9837Epoch 00021: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0516 - acc: 0.9837 - val_loss: 0.0938 - val_acc: 0.9755\n",
      "Epoch 22/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9847Epoch 00022: val_acc improved from 0.97552 to 0.97686, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0479 - acc: 0.9847 - val_loss: 0.0917 - val_acc: 0.9769\n",
      "Epoch 23/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9866Epoch 00023: val_acc improved from 0.97686 to 0.97743, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0435 - acc: 0.9867 - val_loss: 0.0906 - val_acc: 0.9774\n",
      "Epoch 24/100\n",
      "30720/31500 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9874Epoch 00024: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 71us/step - loss: 0.0399 - acc: 0.9874 - val_loss: 0.0937 - val_acc: 0.9770\n",
      "Epoch 25/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9885Epoch 00025: val_acc improved from 0.97743 to 0.97800, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0367 - acc: 0.9884 - val_loss: 0.0903 - val_acc: 0.9780\n",
      "Epoch 26/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9883- ETA:Epoch 00026: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0367 - acc: 0.9883 - val_loss: 0.0924 - val_acc: 0.9772\n",
      "Epoch 27/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9888Epoch 00027: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 71us/step - loss: 0.0349 - acc: 0.9889 - val_loss: 0.0893 - val_acc: 0.9775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9901Epoch 00028: val_acc improved from 0.97800 to 0.97838, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0311 - acc: 0.9901 - val_loss: 0.0934 - val_acc: 0.9784\n",
      "Epoch 29/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9908Epoch 00029: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0302 - acc: 0.9908 - val_loss: 0.0918 - val_acc: 0.9779\n",
      "Epoch 30/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9894Epoch 00030: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0327 - acc: 0.9893 - val_loss: 0.0925 - val_acc: 0.9777\n",
      "Epoch 31/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9909Epoch 00031: val_acc improved from 0.97838 to 0.97886, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0284 - acc: 0.9908 - val_loss: 0.0872 - val_acc: 0.9789\n",
      "Epoch 32/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9925Epoch 00032: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0231 - acc: 0.9925 - val_loss: 0.0902 - val_acc: 0.9787\n",
      "Epoch 33/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9929Epoch 00033: val_acc improved from 0.97886 to 0.97943, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0228 - acc: 0.9929 - val_loss: 0.0888 - val_acc: 0.9794\n",
      "Epoch 34/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9933Epoch 00034: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 74us/step - loss: 0.0219 - acc: 0.9933 - val_loss: 0.0930 - val_acc: 0.9788\n",
      "Epoch 35/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9927- ETA: 0s - loss: 0.022Epoch 00035: val_acc did not improve\n",
      "\n",
      "Epoch 00035: reducing learning rate to 3.2999999166349884e-05.\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0218 - acc: 0.9927 - val_loss: 0.0904 - val_acc: 0.9786\n",
      "Epoch 36/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9948Epoch 00036: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0167 - acc: 0.9948 - val_loss: 0.0907 - val_acc: 0.9792\n",
      "Epoch 37/100\n",
      "31232/31500 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9948Epoch 00037: val_acc improved from 0.97943 to 0.98000, saving model to CNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 2s 73us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0899 - val_acc: 0.9800\n",
      "Epoch 38/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9960Epoch 00038: val_acc did not improve\n",
      "\n",
      "Epoch 00038: reducing learning rate to 1.0890000085055363e-05.\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0141 - acc: 0.9959 - val_loss: 0.0912 - val_acc: 0.9800\n",
      "Epoch 39/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9964Epoch 00039: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0130 - acc: 0.9964 - val_loss: 0.0908 - val_acc: 0.9798\n",
      "Epoch 40/100\n",
      "30976/31500 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9961Epoch 00040: val_acc did not improve\n",
      "31500/31500 [==============================] - 2s 72us/step - loss: 0.0131 - acc: 0.9961 - val_loss: 0.0908 - val_acc: 0.9796\n",
      "Epoch 00040: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'CNN+d'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1124    0    0    2    3    0    0    0    7    0]\n",
      " [   0  956    1    1    1    3    1    2    0    4]\n",
      " [   2    0  986    1    0    1    1    4    1    6]\n",
      " [   2    1    4 1103    1    2    1    3    8    0]\n",
      " [   0    0    0    5 1088    8    2    3    8    0]\n",
      " [   0    4    2    0    8  938    3    4    0    4]\n",
      " [   4    2    3    0    6    4  961    6    5    4]\n",
      " [   2    2   11    6    5    5    6 1069    0    0]\n",
      " [   0    2    3    5    5    1    2    1 1037    0]\n",
      " [   1    4    1    0    0    0    0    0    0 1028]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1136\n",
      "           1       0.98      0.99      0.99       969\n",
      "           2       0.98      0.98      0.98      1002\n",
      "           3       0.98      0.98      0.98      1125\n",
      "           4       0.97      0.98      0.98      1114\n",
      "           5       0.98      0.97      0.97       963\n",
      "           6       0.98      0.97      0.97       995\n",
      "           7       0.98      0.97      0.97      1106\n",
      "           8       0.97      0.98      0.98      1056\n",
      "           9       0.98      0.99      0.99      1034\n",
      "\n",
      "    accuracy                           0.98     10500\n",
      "   macro avg       0.98      0.98      0.98     10500\n",
      "weighted avg       0.98      0.98      0.98     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values ### 0.98085\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN bigger + dropout\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "filters = 100\n",
    "kernel_size = 4\n",
    "input_shape = x_train.shape[1]\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "inputs = Input(shape=(dim,dim,))\n",
    "reshape = Reshape((dim, dim, 1))(inputs)\n",
    "conv1 = Conv2D(filters = filters, kernel_size = kernel_size)(reshape)\n",
    "max1 = MaxPooling2D()(conv1)\n",
    "conv2 = Conv2D(filters = filters, kernel_size = kernel_size)(max1)\n",
    "max2 = MaxPooling2D()(conv2)\n",
    "flat1=Flatten()(max2)\n",
    "dense1 = Dense(units=200, activation='relu')(flat1)\n",
    "drop1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(units=400, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.5)(dense2)\n",
    "dense3 = Dense(units=200, activation='sigmoid')(drop2)\n",
    "output = Dense(units=output_shape, activation='softmax')(dense3)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "reshape_8 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 25, 25, 100)       1700      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 12, 12, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 9, 9, 100)         160100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 200)               320200    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 400)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 644,610\n",
      "Trainable params: 644,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "lr = 1e-4\n",
    "adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31500, 28, 28)\n",
      "Train on 31500 samples, validate on 10500 samples\n",
      "Epoch 1/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 1.8631 - acc: 0.3903Epoch 00001: val_acc improved from -inf to 0.82095, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 5s 152us/step - loss: 1.8628 - acc: 0.3904 - val_loss: 0.7019 - val_acc: 0.8210\n",
      "Epoch 2/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.9473 - acc: 0.7617Epoch 00002: val_acc improved from 0.82095 to 0.90114, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.9472 - acc: 0.7617 - val_loss: 0.3431 - val_acc: 0.9011\n",
      "Epoch 3/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.5626 - acc: 0.8653Epoch 00003: val_acc improved from 0.90114 to 0.93019, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.5626 - acc: 0.8653 - val_loss: 0.2461 - val_acc: 0.9302\n",
      "Epoch 4/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.9083Epoch 00004: val_acc improved from 0.93019 to 0.94552, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.3832 - acc: 0.9084 - val_loss: 0.1985 - val_acc: 0.9455\n",
      "Epoch 5/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2849 - acc: 0.9312Epoch 00005: val_acc improved from 0.94552 to 0.95276, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 124us/step - loss: 0.2849 - acc: 0.9312 - val_loss: 0.1728 - val_acc: 0.9528\n",
      "Epoch 6/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9420Epoch 00006: val_acc improved from 0.95276 to 0.96181, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.2339 - acc: 0.9421 - val_loss: 0.1482 - val_acc: 0.9618\n",
      "Epoch 7/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1933 - acc: 0.9514Epoch 00007: val_acc improved from 0.96181 to 0.96667, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 125us/step - loss: 0.1933 - acc: 0.9515 - val_loss: 0.1345 - val_acc: 0.9667\n",
      "Epoch 8/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1645 - acc: 0.9591Epoch 00008: val_acc improved from 0.96667 to 0.96886, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 123us/step - loss: 0.1644 - acc: 0.9591 - val_loss: 0.1267 - val_acc: 0.9689\n",
      "Epoch 9/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9627Epoch 00009: val_acc improved from 0.96886 to 0.97286, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 124us/step - loss: 0.1472 - acc: 0.9626 - val_loss: 0.1088 - val_acc: 0.9729\n",
      "Epoch 10/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9638Epoch 00010: val_acc improved from 0.97286 to 0.97562, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.1361 - acc: 0.9638 - val_loss: 0.1026 - val_acc: 0.9756\n",
      "Epoch 11/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9665Epoch 00011: val_acc improved from 0.97562 to 0.97676, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.1245 - acc: 0.9665 - val_loss: 0.1015 - val_acc: 0.9768\n",
      "Epoch 12/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1100 - acc: 0.9713Epoch 00012: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.1100 - acc: 0.9713 - val_loss: 0.0991 - val_acc: 0.9768\n",
      "Epoch 13/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9728Epoch 00013: val_acc improved from 0.97676 to 0.97914, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.1025 - acc: 0.9728 - val_loss: 0.0911 - val_acc: 0.9791\n",
      "Epoch 14/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0964 - acc: 0.9748Epoch 00014: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 120us/step - loss: 0.0963 - acc: 0.9748 - val_loss: 0.1004 - val_acc: 0.9769\n",
      "Epoch 15/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0899 - acc: 0.9759Epoch 00015: val_acc improved from 0.97914 to 0.98048, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.0898 - acc: 0.9759 - val_loss: 0.0877 - val_acc: 0.9805\n",
      "Epoch 16/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0778 - acc: 0.9788Epoch 00016: val_acc improved from 0.98048 to 0.98124, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0778 - acc: 0.9788 - val_loss: 0.0817 - val_acc: 0.9812\n",
      "Epoch 17/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9781Epoch 00017: val_acc improved from 0.98124 to 0.98162, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.0792 - acc: 0.9781 - val_loss: 0.0826 - val_acc: 0.9816\n",
      "Epoch 18/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9794Epoch 00018: val_acc improved from 0.98162 to 0.98238, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.0753 - acc: 0.9793 - val_loss: 0.0838 - val_acc: 0.9824\n",
      "Epoch 19/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0710 - acc: 0.9805Epoch 00019: val_acc improved from 0.98238 to 0.98295, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0712 - acc: 0.9805 - val_loss: 0.0816 - val_acc: 0.9830\n",
      "Epoch 20/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0808 - acc: 0.9773Epoch 00020: val_acc improved from 0.98295 to 0.98314, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0808 - acc: 0.9773 - val_loss: 0.0822 - val_acc: 0.9831\n",
      "Epoch 21/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9820Epoch 00021: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 120us/step - loss: 0.0656 - acc: 0.9820 - val_loss: 0.0875 - val_acc: 0.9810\n",
      "Epoch 22/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9827Epoch 00022: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 120us/step - loss: 0.0620 - acc: 0.9827 - val_loss: 0.0817 - val_acc: 0.9826\n",
      "Epoch 23/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9840Epoch 00023: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 120us/step - loss: 0.0570 - acc: 0.9840 - val_loss: 0.0811 - val_acc: 0.9824\n",
      "Epoch 24/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9862Epoch 00024: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0527 - acc: 0.9862 - val_loss: 0.0812 - val_acc: 0.9830\n",
      "Epoch 25/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9870Epoch 00025: val_acc improved from 0.98314 to 0.98476, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0466 - acc: 0.9870 - val_loss: 0.0768 - val_acc: 0.9848\n",
      "Epoch 26/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9862Epoch 00026: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 120us/step - loss: 0.0502 - acc: 0.9862 - val_loss: 0.0800 - val_acc: 0.9846\n",
      "Epoch 27/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9856Epoch 00027: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0486 - acc: 0.9857 - val_loss: 0.0753 - val_acc: 0.9842\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9872Epoch 00028: val_acc improved from 0.98476 to 0.98486, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0459 - acc: 0.9872 - val_loss: 0.0759 - val_acc: 0.9849\n",
      "Epoch 29/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9880Epoch 00029: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0432 - acc: 0.9880 - val_loss: 0.0872 - val_acc: 0.9826\n",
      "Epoch 30/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9879Epoch 00030: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0412 - acc: 0.9879 - val_loss: 0.0762 - val_acc: 0.9838\n",
      "Epoch 31/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9890Epoch 00031: val_acc did not improve\n",
      "\n",
      "Epoch 00031: reducing learning rate to 3.2999999166349884e-05.\n",
      "31500/31500 [==============================] - 4s 124us/step - loss: 0.0372 - acc: 0.9890 - val_loss: 0.0784 - val_acc: 0.9846\n",
      "Epoch 32/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9912Epoch 00032: val_acc improved from 0.98486 to 0.98590, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0331 - acc: 0.9912 - val_loss: 0.0733 - val_acc: 0.9859\n",
      "Epoch 33/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9915Epoch 00033: val_acc improved from 0.98590 to 0.98600, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0303 - acc: 0.9915 - val_loss: 0.0752 - val_acc: 0.9860\n",
      "Epoch 34/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9928Epoch 00034: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0274 - acc: 0.9928 - val_loss: 0.0750 - val_acc: 0.9859\n",
      "Epoch 35/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9925Epoch 00035: val_acc improved from 0.98600 to 0.98619, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.0266 - acc: 0.9925 - val_loss: 0.0744 - val_acc: 0.9862\n",
      "Epoch 36/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9928Epoch 00036: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0270 - acc: 0.9928 - val_loss: 0.0725 - val_acc: 0.9859\n",
      "Epoch 37/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9931Epoch 00037: val_acc improved from 0.98619 to 0.98638, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0254 - acc: 0.9931 - val_loss: 0.0732 - val_acc: 0.9864\n",
      "Epoch 38/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9938Epoch 00038: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0225 - acc: 0.9938 - val_loss: 0.0769 - val_acc: 0.9853\n",
      "Epoch 39/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9934Epoch 00039: val_acc improved from 0.98638 to 0.98705, saving model to bCNN+d/weights.hdf5\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0243 - acc: 0.9934 - val_loss: 0.0739 - val_acc: 0.9870\n",
      "Epoch 40/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9937Epoch 00040: val_acc did not improve\n",
      "\n",
      "Epoch 00040: reducing learning rate to 1.0890000085055363e-05.\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.0243 - acc: 0.9937 - val_loss: 0.0748 - val_acc: 0.9866\n",
      "Epoch 41/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9939Epoch 00041: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 122us/step - loss: 0.0223 - acc: 0.9939 - val_loss: 0.0750 - val_acc: 0.9860\n",
      "Epoch 42/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9948Epoch 00042: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0213 - acc: 0.9948 - val_loss: 0.0748 - val_acc: 0.9862\n",
      "Epoch 43/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9946Epoch 00043: val_acc did not improve\n",
      "\n",
      "Epoch 00043: reducing learning rate to 3.59370011210558e-06.\n",
      "31500/31500 [==============================] - 4s 121us/step - loss: 0.0204 - acc: 0.9946 - val_loss: 0.0740 - val_acc: 0.9860\n",
      "Epoch 44/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9947Epoch 00044: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 120us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.0733 - val_acc: 0.9864\n",
      "Epoch 45/100\n",
      "31488/31500 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9940Epoch 00045: val_acc did not improve\n",
      "31500/31500 [==============================] - 4s 120us/step - loss: 0.0210 - acc: 0.9940 - val_loss: 0.0731 - val_acc: 0.9864\n",
      "Epoch 00045: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "path = 'bCNN+d'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "checkpoint = ModelCheckpoint(path+'/weights.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=9, verbose = 1)\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss',factor=0.33,patience=3,verbose = 1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],dim,dim)\n",
    "x_test = x_test.reshape(x_test.shape[0],dim,dim)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, \n",
    "                    callbacks = [checkpoint, early_stopping, reduceLR],validation_data=(x_test, y_test), class_weight='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = path + '/weights.hdf5'\n",
    "model.load_weights(weight_file)\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1126    1    0    5    1    1    0    0    1    1]\n",
      " [   0  961    1    0    0    0    1    3    1    2]\n",
      " [   1    0  990    2    0    0    1    3    0    5]\n",
      " [   1    0    2 1110    1    0    2    2    7    0]\n",
      " [   1    0    0    3 1095    5    0    2    8    0]\n",
      " [   0    2    0    1    4  946    4    2    0    4]\n",
      " [   1    1    3    0    2    0  979    5    4    0]\n",
      " [   1    1    3    2    2    2    3 1092    0    0]\n",
      " [   2    1    2    4    3    0    0    1 1042    1]\n",
      " [   1    7    0    0    0    0    3    0    0 1023]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1136\n",
      "           1       0.99      0.99      0.99       969\n",
      "           2       0.99      0.99      0.99      1002\n",
      "           3       0.98      0.99      0.99      1125\n",
      "           4       0.99      0.98      0.99      1114\n",
      "           5       0.99      0.98      0.99       963\n",
      "           6       0.99      0.98      0.98       995\n",
      "           7       0.98      0.99      0.99      1106\n",
      "           8       0.98      0.99      0.98      1056\n",
      "           9       0.99      0.99      0.99      1034\n",
      "\n",
      "    accuracy                           0.99     10500\n",
      "   macro avg       0.99      0.99      0.99     10500\n",
      "weighted avg       0.99      0.99      0.99     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv').values ### 0.98757\n",
    "test_df = test_df.reshape(test_df.shape[0],dim,dim)\n",
    "generate_submission(model, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
